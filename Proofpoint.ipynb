{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proofpoint.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "D8xFuWWZ0Z5A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mounting drive and adjustments"
      ]
    },
    {
      "metadata": {
        "id": "wjns-AOa40fA",
        "colab_type": "code",
        "outputId": "7e3c3e4c-2989-4fd3-a421-d744077a1292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NYtc4ArV5RfK",
        "colab_type": "code",
        "outputId": "3a6de21e-9b3c-4481-9d5e-64f25a608dee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#Change working directory to make it easier to access the files\n",
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/proofpoint\")\n",
        "os.getcwd() "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks/proofpoint'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "3xzV--qd5o5m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adamax\n",
        "from keras.callbacks import ModelCheckpoint\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yESn77mP0uIT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the data"
      ]
    },
    {
      "metadata": {
        "id": "_TwvH9QV5Vfk",
        "colab_type": "code",
        "outputId": "5ac69854-a698-45ce-d1b3-cabb370f3b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "features_dataset=pd.read_csv('train_features.csv', header=None)\n",
        "labels_dataset=pd.read_csv('train_labels.csv', header=None)\n",
        "print('Features dataset shape is: ' + str(features_dataset.shape))\n",
        "print('Labels dataset shape is: '+ str(labels_dataset.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features dataset shape is: (140, 903)\n",
            "Labels dataset shape is: (140, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z1Lw2HgQ0zsI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data exploration"
      ]
    },
    {
      "metadata": {
        "id": "nLwuOCP752V_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "a44d572f-3c44-4ee8-ccd3-d98a1882d6f3"
      },
      "cell_type": "code",
      "source": [
        "features_dataset.describe()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>...</th>\n",
              "      <th>893</th>\n",
              "      <th>894</th>\n",
              "      <th>895</th>\n",
              "      <th>896</th>\n",
              "      <th>897</th>\n",
              "      <th>898</th>\n",
              "      <th>899</th>\n",
              "      <th>900</th>\n",
              "      <th>901</th>\n",
              "      <th>902</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>140.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-3.706226</td>\n",
              "      <td>0.620541</td>\n",
              "      <td>-0.691226</td>\n",
              "      <td>-2.883438</td>\n",
              "      <td>0.798445</td>\n",
              "      <td>0.276561</td>\n",
              "      <td>0.382528</td>\n",
              "      <td>-0.890959</td>\n",
              "      <td>-1.694734</td>\n",
              "      <td>-0.719966</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.508409</td>\n",
              "      <td>-1.576743</td>\n",
              "      <td>0.190566</td>\n",
              "      <td>0.665053</td>\n",
              "      <td>-0.774684</td>\n",
              "      <td>0.931779</td>\n",
              "      <td>0.791181</td>\n",
              "      <td>-0.458641</td>\n",
              "      <td>0.723416</td>\n",
              "      <td>-0.132218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.064704</td>\n",
              "      <td>3.014747</td>\n",
              "      <td>3.295262</td>\n",
              "      <td>3.331956</td>\n",
              "      <td>3.484214</td>\n",
              "      <td>3.203680</td>\n",
              "      <td>3.114446</td>\n",
              "      <td>3.221021</td>\n",
              "      <td>3.639898</td>\n",
              "      <td>3.124935</td>\n",
              "      <td>...</td>\n",
              "      <td>3.125331</td>\n",
              "      <td>3.262723</td>\n",
              "      <td>3.429600</td>\n",
              "      <td>3.092077</td>\n",
              "      <td>3.462897</td>\n",
              "      <td>3.301253</td>\n",
              "      <td>3.131848</td>\n",
              "      <td>2.616963</td>\n",
              "      <td>3.382829</td>\n",
              "      <td>3.657816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-12.753200</td>\n",
              "      <td>-8.020900</td>\n",
              "      <td>-9.688800</td>\n",
              "      <td>-10.540100</td>\n",
              "      <td>-8.493400</td>\n",
              "      <td>-6.525400</td>\n",
              "      <td>-8.397000</td>\n",
              "      <td>-8.372300</td>\n",
              "      <td>-10.153200</td>\n",
              "      <td>-10.185400</td>\n",
              "      <td>...</td>\n",
              "      <td>-8.152700</td>\n",
              "      <td>-10.229800</td>\n",
              "      <td>-8.713400</td>\n",
              "      <td>-9.106600</td>\n",
              "      <td>-11.294900</td>\n",
              "      <td>-7.695700</td>\n",
              "      <td>-7.090500</td>\n",
              "      <td>-6.851600</td>\n",
              "      <td>-6.184400</td>\n",
              "      <td>-9.723500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-6.393700</td>\n",
              "      <td>-1.665500</td>\n",
              "      <td>-2.625975</td>\n",
              "      <td>-5.215000</td>\n",
              "      <td>-1.934025</td>\n",
              "      <td>-1.833875</td>\n",
              "      <td>-1.871550</td>\n",
              "      <td>-2.941725</td>\n",
              "      <td>-3.986725</td>\n",
              "      <td>-2.876275</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.739425</td>\n",
              "      <td>-3.837325</td>\n",
              "      <td>-2.248525</td>\n",
              "      <td>-1.361275</td>\n",
              "      <td>-3.141325</td>\n",
              "      <td>-0.960575</td>\n",
              "      <td>-1.299400</td>\n",
              "      <td>-2.491375</td>\n",
              "      <td>-1.769575</td>\n",
              "      <td>-2.728575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-4.022400</td>\n",
              "      <td>0.559550</td>\n",
              "      <td>-0.651150</td>\n",
              "      <td>-3.022050</td>\n",
              "      <td>1.005600</td>\n",
              "      <td>0.309250</td>\n",
              "      <td>0.454450</td>\n",
              "      <td>-0.826550</td>\n",
              "      <td>-1.664150</td>\n",
              "      <td>-0.940650</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.873850</td>\n",
              "      <td>-1.594600</td>\n",
              "      <td>0.274000</td>\n",
              "      <td>0.456750</td>\n",
              "      <td>-1.232550</td>\n",
              "      <td>1.210100</td>\n",
              "      <td>0.785650</td>\n",
              "      <td>-0.055900</td>\n",
              "      <td>0.350250</td>\n",
              "      <td>0.200050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>-0.860025</td>\n",
              "      <td>2.784550</td>\n",
              "      <td>1.432475</td>\n",
              "      <td>-0.441100</td>\n",
              "      <td>3.353100</td>\n",
              "      <td>2.432525</td>\n",
              "      <td>2.270475</td>\n",
              "      <td>1.324300</td>\n",
              "      <td>0.575900</td>\n",
              "      <td>1.441625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.490925</td>\n",
              "      <td>0.554300</td>\n",
              "      <td>2.449825</td>\n",
              "      <td>2.790450</td>\n",
              "      <td>1.842575</td>\n",
              "      <td>3.121575</td>\n",
              "      <td>2.618375</td>\n",
              "      <td>1.136900</td>\n",
              "      <td>3.384000</td>\n",
              "      <td>2.175950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.862500</td>\n",
              "      <td>9.077900</td>\n",
              "      <td>7.729800</td>\n",
              "      <td>5.230100</td>\n",
              "      <td>10.856000</td>\n",
              "      <td>8.125200</td>\n",
              "      <td>9.032900</td>\n",
              "      <td>8.249100</td>\n",
              "      <td>7.994100</td>\n",
              "      <td>7.576900</td>\n",
              "      <td>...</td>\n",
              "      <td>7.073000</td>\n",
              "      <td>6.822000</td>\n",
              "      <td>11.433600</td>\n",
              "      <td>9.259000</td>\n",
              "      <td>7.385900</td>\n",
              "      <td>12.639000</td>\n",
              "      <td>9.713000</td>\n",
              "      <td>5.019900</td>\n",
              "      <td>8.439900</td>\n",
              "      <td>9.632300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 900 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              3           4           5           6           7           8    \\\n",
              "count  140.000000  140.000000  140.000000  140.000000  140.000000  140.000000   \n",
              "mean    -3.706226    0.620541   -0.691226   -2.883438    0.798445    0.276561   \n",
              "std      4.064704    3.014747    3.295262    3.331956    3.484214    3.203680   \n",
              "min    -12.753200   -8.020900   -9.688800  -10.540100   -8.493400   -6.525400   \n",
              "25%     -6.393700   -1.665500   -2.625975   -5.215000   -1.934025   -1.833875   \n",
              "50%     -4.022400    0.559550   -0.651150   -3.022050    1.005600    0.309250   \n",
              "75%     -0.860025    2.784550    1.432475   -0.441100    3.353100    2.432525   \n",
              "max      7.862500    9.077900    7.729800    5.230100   10.856000    8.125200   \n",
              "\n",
              "              9           10          11          12      ...             893  \\\n",
              "count  140.000000  140.000000  140.000000  140.000000     ...      140.000000   \n",
              "mean     0.382528   -0.890959   -1.694734   -0.719966     ...       -1.508409   \n",
              "std      3.114446    3.221021    3.639898    3.124935     ...        3.125331   \n",
              "min     -8.397000   -8.372300  -10.153200  -10.185400     ...       -8.152700   \n",
              "25%     -1.871550   -2.941725   -3.986725   -2.876275     ...       -3.739425   \n",
              "50%      0.454450   -0.826550   -1.664150   -0.940650     ...       -1.873850   \n",
              "75%      2.270475    1.324300    0.575900    1.441625     ...        0.490925   \n",
              "max      9.032900    8.249100    7.994100    7.576900     ...        7.073000   \n",
              "\n",
              "              894         895         896         897         898         899  \\\n",
              "count  140.000000  140.000000  140.000000  140.000000  140.000000  140.000000   \n",
              "mean    -1.576743    0.190566    0.665053   -0.774684    0.931779    0.791181   \n",
              "std      3.262723    3.429600    3.092077    3.462897    3.301253    3.131848   \n",
              "min    -10.229800   -8.713400   -9.106600  -11.294900   -7.695700   -7.090500   \n",
              "25%     -3.837325   -2.248525   -1.361275   -3.141325   -0.960575   -1.299400   \n",
              "50%     -1.594600    0.274000    0.456750   -1.232550    1.210100    0.785650   \n",
              "75%      0.554300    2.449825    2.790450    1.842575    3.121575    2.618375   \n",
              "max      6.822000   11.433600    9.259000    7.385900   12.639000    9.713000   \n",
              "\n",
              "              900         901         902  \n",
              "count  140.000000  140.000000  140.000000  \n",
              "mean    -0.458641    0.723416   -0.132218  \n",
              "std      2.616963    3.382829    3.657816  \n",
              "min     -6.851600   -6.184400   -9.723500  \n",
              "25%     -2.491375   -1.769575   -2.728575  \n",
              "50%     -0.055900    0.350250    0.200050  \n",
              "75%      1.136900    3.384000    2.175950  \n",
              "max      5.019900    8.439900    9.632300  \n",
              "\n",
              "[8 rows x 900 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "xQqbvFvP7sgZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "3ca1415c-ceb7-46ac-80cc-50c1609751a2"
      },
      "cell_type": "code",
      "source": [
        "features_dataset.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>893</th>\n",
              "      <th>894</th>\n",
              "      <th>895</th>\n",
              "      <th>896</th>\n",
              "      <th>897</th>\n",
              "      <th>898</th>\n",
              "      <th>899</th>\n",
              "      <th>900</th>\n",
              "      <th>901</th>\n",
              "      <th>902</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>right</td>\n",
              "      <td>type_3</td>\n",
              "      <td>green</td>\n",
              "      <td>0.8827</td>\n",
              "      <td>3.2917</td>\n",
              "      <td>1.4651</td>\n",
              "      <td>-4.5278</td>\n",
              "      <td>1.0490</td>\n",
              "      <td>2.3398</td>\n",
              "      <td>1.8729</td>\n",
              "      <td>...</td>\n",
              "      <td>0.8854</td>\n",
              "      <td>-0.8752</td>\n",
              "      <td>0.2662</td>\n",
              "      <td>4.9686</td>\n",
              "      <td>2.5588</td>\n",
              "      <td>-0.6121</td>\n",
              "      <td>-3.3692</td>\n",
              "      <td>0.0550</td>\n",
              "      <td>-1.5296</td>\n",
              "      <td>-5.3041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>left</td>\n",
              "      <td>type_2</td>\n",
              "      <td>blue</td>\n",
              "      <td>1.4458</td>\n",
              "      <td>-2.9575</td>\n",
              "      <td>-1.2341</td>\n",
              "      <td>-3.9684</td>\n",
              "      <td>-2.7645</td>\n",
              "      <td>5.6346</td>\n",
              "      <td>1.7838</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.3774</td>\n",
              "      <td>-2.2512</td>\n",
              "      <td>-1.6331</td>\n",
              "      <td>7.2724</td>\n",
              "      <td>1.7616</td>\n",
              "      <td>4.2826</td>\n",
              "      <td>5.5557</td>\n",
              "      <td>1.0588</td>\n",
              "      <td>2.6734</td>\n",
              "      <td>-4.5224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>right</td>\n",
              "      <td>type_2</td>\n",
              "      <td>green</td>\n",
              "      <td>-10.2982</td>\n",
              "      <td>-0.3714</td>\n",
              "      <td>-0.9886</td>\n",
              "      <td>-3.2219</td>\n",
              "      <td>4.0925</td>\n",
              "      <td>-0.8319</td>\n",
              "      <td>-3.0588</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.4408</td>\n",
              "      <td>-2.5580</td>\n",
              "      <td>-1.2116</td>\n",
              "      <td>5.1098</td>\n",
              "      <td>-0.6747</td>\n",
              "      <td>-1.2528</td>\n",
              "      <td>-2.2944</td>\n",
              "      <td>-3.4745</td>\n",
              "      <td>2.8633</td>\n",
              "      <td>1.6737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>right</td>\n",
              "      <td>type_1</td>\n",
              "      <td>red</td>\n",
              "      <td>-8.4566</td>\n",
              "      <td>-0.2408</td>\n",
              "      <td>-3.0342</td>\n",
              "      <td>2.9534</td>\n",
              "      <td>2.8977</td>\n",
              "      <td>0.8851</td>\n",
              "      <td>3.0113</td>\n",
              "      <td>...</td>\n",
              "      <td>0.9434</td>\n",
              "      <td>-0.8771</td>\n",
              "      <td>0.4143</td>\n",
              "      <td>4.3368</td>\n",
              "      <td>-11.2949</td>\n",
              "      <td>-7.4289</td>\n",
              "      <td>7.9900</td>\n",
              "      <td>-6.2433</td>\n",
              "      <td>1.6592</td>\n",
              "      <td>-4.8601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>left</td>\n",
              "      <td>type_1</td>\n",
              "      <td>blue</td>\n",
              "      <td>4.2681</td>\n",
              "      <td>-2.2052</td>\n",
              "      <td>-5.9093</td>\n",
              "      <td>0.1036</td>\n",
              "      <td>1.8462</td>\n",
              "      <td>1.9801</td>\n",
              "      <td>2.1129</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.2290</td>\n",
              "      <td>0.5390</td>\n",
              "      <td>0.7648</td>\n",
              "      <td>1.1348</td>\n",
              "      <td>1.9829</td>\n",
              "      <td>3.7682</td>\n",
              "      <td>-1.7092</td>\n",
              "      <td>1.4791</td>\n",
              "      <td>5.7732</td>\n",
              "      <td>-3.9106</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 903 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0       1      2        3       4       5       6       7       8    \\\n",
              "0  right  type_3  green   0.8827  3.2917  1.4651 -4.5278  1.0490  2.3398   \n",
              "1   left  type_2   blue   1.4458 -2.9575 -1.2341 -3.9684 -2.7645  5.6346   \n",
              "2  right  type_2  green -10.2982 -0.3714 -0.9886 -3.2219  4.0925 -0.8319   \n",
              "3  right  type_1    red  -8.4566 -0.2408 -3.0342  2.9534  2.8977  0.8851   \n",
              "4   left  type_1   blue   4.2681 -2.2052 -5.9093  0.1036  1.8462  1.9801   \n",
              "\n",
              "      9     ...       893     894     895     896      897     898     899  \\\n",
              "0  1.8729   ...    0.8854 -0.8752  0.2662  4.9686   2.5588 -0.6121 -3.3692   \n",
              "1  1.7838   ...   -4.3774 -2.2512 -1.6331  7.2724   1.7616  4.2826  5.5557   \n",
              "2 -3.0588   ...   -1.4408 -2.5580 -1.2116  5.1098  -0.6747 -1.2528 -2.2944   \n",
              "3  3.0113   ...    0.9434 -0.8771  0.4143  4.3368 -11.2949 -7.4289  7.9900   \n",
              "4  2.1129   ...   -0.2290  0.5390  0.7648  1.1348   1.9829  3.7682 -1.7092   \n",
              "\n",
              "      900     901     902  \n",
              "0  0.0550 -1.5296 -5.3041  \n",
              "1  1.0588  2.6734 -4.5224  \n",
              "2 -3.4745  2.8633  1.6737  \n",
              "3 -6.2433  1.6592 -4.8601  \n",
              "4  1.4791  5.7732 -3.9106  \n",
              "\n",
              "[5 rows x 903 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "KmNyNQIA7tni",
        "colab_type": "code",
        "outputId": "c1d19070-360a-42af-a66c-69530f25b632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "features_dataset.isnull().values.any() #check for missing values"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "U9v45VdEZKcq",
        "colab_type": "code",
        "outputId": "9e29a07d-8e59-4e4b-ba03-92906fbf3d67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "cell_type": "code",
      "source": [
        "#Counting values for categorical\n",
        "from collections import Counter\n",
        "print('first col')\n",
        "print(Counter(features_dataset[0]).keys())\n",
        "print(Counter(features_dataset[0]).values())\n",
        "print('second col')\n",
        "print(Counter(features_dataset[1]).keys())\n",
        "print(Counter(features_dataset[1]).values())\n",
        "print('third col')\n",
        "print(Counter(features_dataset[2]).keys())\n",
        "print(Counter(features_dataset[2]).values())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first col\n",
            "dict_keys(['right', 'left'])\n",
            "dict_values([61, 79])\n",
            "second col\n",
            "dict_keys(['type_3', 'type_2', 'type_1'])\n",
            "dict_values([64, 32, 44])\n",
            "third col\n",
            "dict_keys(['green', 'blue', 'red'])\n",
            "dict_values([51, 36, 53])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "neMt0wIm1hjl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convert categorical and labels to one-hot"
      ]
    },
    {
      "metadata": {
        "id": "HgHY0v7t-cM1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#converts the labels to one-hot\n",
        "y_cat = to_categorical(labels_dataset.iloc[:,:]) \n",
        "\n",
        "# Get one hot encoding of columns 1-3 in the feature dataset\n",
        "one_hot = pd.get_dummies(features_dataset.iloc[:,:3])\n",
        "\n",
        "# Drop columns 1-3 as it is now encoded\n",
        "features_dataset = features_dataset.drop(features_dataset.iloc[:,:3],axis = 1)\n",
        "\n",
        "# Join the encoded df\n",
        "features_dataset = features_dataset.join(one_hot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "__-tFMaoVnVV",
        "colab_type": "code",
        "outputId": "177de5f9-4d6e-42cb-d38a-ee38573d3cf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#Check new shape\n",
        "features_dataset.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(140, 908)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "WtoWCgI610yk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare Test and Train Data\n"
      ]
    },
    {
      "metadata": {
        "id": "nU7jUcZ75n8R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features, labels = features_dataset, y_cat\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(\n",
        "        features, labels,\n",
        "        train_size=0.7, #70% training and 30% testing \n",
        "        test_size=0.3,\n",
        "        random_state=4, #Allow for reproducible results \n",
        "        # keep same proportion of 'target' in test and target data\n",
        "        stratify=labels\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VTnRWvE32DAU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Grid search for hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "c-A05JfG--ZW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Tune the batch size and epochs\n",
        "\n",
        "#Define model\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(5,input_shape=(908,), activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(7, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "  #Compile\n",
        "  model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
        "# define the grid search parameters for Batch Size and Number of Epochs\n",
        "batch_size = [5, 10, 15, 20, 25]\n",
        "epochs = [10, 20, 30, 40]\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(train_features, train_labels, validation_data=(test_features, test_labels),verbose=1)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tpyAxPCSfh7y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Tune the Training Optimization Algorithm\n",
        "\n",
        "def create_model(optimizer='adam'):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(5,input_shape=(908,), activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(7, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "  #Compile\n",
        "  model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, epochs=30, batch_size=15, verbose=0)\n",
        "\n",
        "# define the grid search parameters for Batch Size and Number of Epochs\n",
        "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "param_grid = dict(optimizer=optimizer)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(train_features, train_labels, validation_data=(test_features, test_labels),verbose=1)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vk_4kZe9iD6x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Tune the Number of Neurons in the Hidden Layer\n",
        "\n",
        "def create_model(neurons1=1,neurons2=1):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(neurons1,input_shape=(908,), activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(neurons2, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "  \n",
        "  #Compile\n",
        "  model.compile('Adamax', 'categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, epochs=20, batch_size=15, verbose=0)\n",
        "\n",
        "# define the grid search parameters\n",
        "neurons1 = [1,2,3,4 ,5,6,7,8,9, 10, 15]\n",
        "neurons2 = [1,2,3,4 ,5,6,7,8,9, 10, 15]\n",
        "\n",
        "param_grid = dict(neurons1=neurons1, neurons2=neurons2 )\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(train_features, train_labels, validation_data=(test_features, test_labels),verbose=0)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "blQVBdJm_bUU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Run the model with the optimal parameters\n",
        "model = Sequential()\n",
        "model.add(Dense(5,input_shape=(908,), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "#Compile\n",
        "model.compile('RMSprop', 'categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# checkpoint\n",
        "filepath=\"weights.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SCSDeak8_fdT",
        "colab_type": "code",
        "outputId": "1d300668-a6bf-4941-9237-e8ca025ce485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14654
        }
      },
      "cell_type": "code",
      "source": [
        "#fit\n",
        "history=model.fit(train_features, train_labels, validation_data=(test_features, test_labels),verbose=1,epochs=201, callbacks=callbacks_list) "
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 35 samples\n",
            "Epoch 1/201\n",
            "105/105 [==============================] - 3s 25ms/step - loss: 2.7632 - acc: 0.3429 - val_loss: 1.4985 - val_acc: 0.4857\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.48571, saving model to weights.best.hdf5\n",
            "Epoch 2/201\n",
            "105/105 [==============================] - 0s 663us/step - loss: 2.3109 - acc: 0.4667 - val_loss: 1.3403 - val_acc: 0.4857\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.48571\n",
            "Epoch 3/201\n",
            "105/105 [==============================] - 0s 454us/step - loss: 1.9055 - acc: 0.5429 - val_loss: 1.1973 - val_acc: 0.4857\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.48571\n",
            "Epoch 4/201\n",
            "105/105 [==============================] - 0s 345us/step - loss: 1.4295 - acc: 0.5143 - val_loss: 1.2094 - val_acc: 0.5143\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.48571 to 0.51429, saving model to weights.best.hdf5\n",
            "Epoch 5/201\n",
            "105/105 [==============================] - 0s 355us/step - loss: 1.2025 - acc: 0.5143 - val_loss: 1.2364 - val_acc: 0.5143\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.51429\n",
            "Epoch 6/201\n",
            "105/105 [==============================] - 0s 327us/step - loss: 1.0319 - acc: 0.5810 - val_loss: 1.3236 - val_acc: 0.5429\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.51429 to 0.54286, saving model to weights.best.hdf5\n",
            "Epoch 7/201\n",
            "105/105 [==============================] - 0s 455us/step - loss: 1.1906 - acc: 0.5810 - val_loss: 1.2372 - val_acc: 0.5143\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.54286\n",
            "Epoch 8/201\n",
            "105/105 [==============================] - 0s 397us/step - loss: 1.1220 - acc: 0.5333 - val_loss: 1.2041 - val_acc: 0.5429\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.54286\n",
            "Epoch 9/201\n",
            "105/105 [==============================] - 0s 440us/step - loss: 1.1149 - acc: 0.5714 - val_loss: 1.2330 - val_acc: 0.5429\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.54286\n",
            "Epoch 10/201\n",
            "105/105 [==============================] - 0s 432us/step - loss: 0.9948 - acc: 0.5143 - val_loss: 1.2452 - val_acc: 0.5429\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.54286\n",
            "Epoch 11/201\n",
            "105/105 [==============================] - 0s 458us/step - loss: 0.9955 - acc: 0.5619 - val_loss: 1.2509 - val_acc: 0.5429\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.54286\n",
            "Epoch 12/201\n",
            "105/105 [==============================] - 0s 449us/step - loss: 0.9451 - acc: 0.6095 - val_loss: 1.2953 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.54286 to 0.57143, saving model to weights.best.hdf5\n",
            "Epoch 13/201\n",
            "105/105 [==============================] - 0s 368us/step - loss: 0.9285 - acc: 0.5619 - val_loss: 1.3242 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.57143\n",
            "Epoch 14/201\n",
            "105/105 [==============================] - 0s 437us/step - loss: 0.9076 - acc: 0.5143 - val_loss: 1.4567 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.57143\n",
            "Epoch 15/201\n",
            "105/105 [==============================] - 0s 485us/step - loss: 0.8239 - acc: 0.5714 - val_loss: 1.4206 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.57143\n",
            "Epoch 16/201\n",
            "105/105 [==============================] - 0s 438us/step - loss: 0.9967 - acc: 0.5333 - val_loss: 1.3406 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.57143\n",
            "Epoch 17/201\n",
            "105/105 [==============================] - 0s 395us/step - loss: 0.8821 - acc: 0.5429 - val_loss: 1.3529 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.57143\n",
            "Epoch 18/201\n",
            "105/105 [==============================] - 0s 909us/step - loss: 0.8509 - acc: 0.5429 - val_loss: 1.4336 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.57143\n",
            "Epoch 19/201\n",
            "105/105 [==============================] - 0s 641us/step - loss: 0.8642 - acc: 0.5524 - val_loss: 1.4675 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.57143\n",
            "Epoch 20/201\n",
            "105/105 [==============================] - 0s 472us/step - loss: 0.8270 - acc: 0.5714 - val_loss: 1.5223 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.57143\n",
            "Epoch 21/201\n",
            "105/105 [==============================] - 0s 626us/step - loss: 0.8882 - acc: 0.5714 - val_loss: 1.3364 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.57143\n",
            "Epoch 22/201\n",
            "105/105 [==============================] - 0s 728us/step - loss: 0.7985 - acc: 0.5810 - val_loss: 1.2585 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.57143\n",
            "Epoch 23/201\n",
            "105/105 [==============================] - 0s 652us/step - loss: 0.7875 - acc: 0.6095 - val_loss: 1.2855 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.57143\n",
            "Epoch 24/201\n",
            "105/105 [==============================] - 0s 617us/step - loss: 0.8106 - acc: 0.6000 - val_loss: 1.4377 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.57143\n",
            "Epoch 25/201\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 1.0732 - acc: 0.4762 - val_loss: 1.2344 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.57143\n",
            "Epoch 26/201\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.8163 - acc: 0.5810 - val_loss: 1.2751 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.57143\n",
            "Epoch 27/201\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.7671 - acc: 0.5714 - val_loss: 1.3018 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.57143\n",
            "Epoch 28/201\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.9845 - acc: 0.5429 - val_loss: 1.1944 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.57143\n",
            "Epoch 29/201\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.8161 - acc: 0.5810 - val_loss: 1.2816 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.57143\n",
            "Epoch 30/201\n",
            "105/105 [==============================] - 0s 833us/step - loss: 0.8430 - acc: 0.5238 - val_loss: 1.3251 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.57143\n",
            "Epoch 31/201\n",
            "105/105 [==============================] - 0s 458us/step - loss: 0.7698 - acc: 0.6000 - val_loss: 1.3925 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.57143\n",
            "Epoch 32/201\n",
            "105/105 [==============================] - 0s 434us/step - loss: 0.7866 - acc: 0.5524 - val_loss: 1.3080 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.57143\n",
            "Epoch 33/201\n",
            "105/105 [==============================] - 0s 443us/step - loss: 0.7164 - acc: 0.6095 - val_loss: 1.3443 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.57143\n",
            "Epoch 34/201\n",
            "105/105 [==============================] - 0s 495us/step - loss: 0.8006 - acc: 0.5333 - val_loss: 1.3562 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.57143\n",
            "Epoch 35/201\n",
            "105/105 [==============================] - 0s 465us/step - loss: 0.7107 - acc: 0.5524 - val_loss: 1.4156 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.57143\n",
            "Epoch 36/201\n",
            "105/105 [==============================] - 0s 438us/step - loss: 0.7510 - acc: 0.6000 - val_loss: 1.5011 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00036: val_acc improved from 0.57143 to 0.60000, saving model to weights.best.hdf5\n",
            "Epoch 37/201\n",
            "105/105 [==============================] - 0s 549us/step - loss: 0.7255 - acc: 0.6000 - val_loss: 1.6137 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.60000\n",
            "Epoch 38/201\n",
            "105/105 [==============================] - 0s 724us/step - loss: 0.7557 - acc: 0.5619 - val_loss: 1.3292 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.60000\n",
            "Epoch 39/201\n",
            "105/105 [==============================] - 0s 533us/step - loss: 0.7771 - acc: 0.5714 - val_loss: 1.3692 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.60000\n",
            "Epoch 40/201\n",
            "105/105 [==============================] - 0s 533us/step - loss: 0.7675 - acc: 0.6095 - val_loss: 1.6371 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.60000\n",
            "Epoch 41/201\n",
            "105/105 [==============================] - 0s 830us/step - loss: 0.7357 - acc: 0.5714 - val_loss: 1.3405 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.60000\n",
            "Epoch 42/201\n",
            "105/105 [==============================] - 0s 793us/step - loss: 0.7275 - acc: 0.6095 - val_loss: 1.5633 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.60000\n",
            "Epoch 43/201\n",
            "105/105 [==============================] - 0s 614us/step - loss: 0.7563 - acc: 0.5619 - val_loss: 1.6118 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.60000\n",
            "Epoch 44/201\n",
            "105/105 [==============================] - 0s 767us/step - loss: 0.7876 - acc: 0.5429 - val_loss: 1.5520 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.60000\n",
            "Epoch 45/201\n",
            "105/105 [==============================] - 0s 450us/step - loss: 0.6721 - acc: 0.6095 - val_loss: 1.7449 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.60000\n",
            "Epoch 46/201\n",
            "105/105 [==============================] - 0s 730us/step - loss: 0.7259 - acc: 0.5619 - val_loss: 1.3770 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.60000\n",
            "Epoch 47/201\n",
            "105/105 [==============================] - 0s 461us/step - loss: 0.7098 - acc: 0.5810 - val_loss: 1.4748 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.60000\n",
            "Epoch 48/201\n",
            "105/105 [==============================] - 0s 710us/step - loss: 0.6994 - acc: 0.5714 - val_loss: 1.6457 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.60000\n",
            "Epoch 49/201\n",
            "105/105 [==============================] - 0s 572us/step - loss: 0.7605 - acc: 0.6381 - val_loss: 1.5277 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00049: val_acc improved from 0.60000 to 0.60000, saving model to weights.best.hdf5\n",
            "Epoch 50/201\n",
            "105/105 [==============================] - 0s 457us/step - loss: 0.7082 - acc: 0.6095 - val_loss: 1.5279 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.60000\n",
            "Epoch 51/201\n",
            "105/105 [==============================] - 0s 801us/step - loss: 0.6829 - acc: 0.6286 - val_loss: 1.6358 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.60000\n",
            "Epoch 52/201\n",
            "105/105 [==============================] - 0s 523us/step - loss: 0.7097 - acc: 0.6095 - val_loss: 1.3707 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.60000\n",
            "Epoch 53/201\n",
            "105/105 [==============================] - 0s 545us/step - loss: 0.6876 - acc: 0.6286 - val_loss: 1.4376 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.60000\n",
            "Epoch 54/201\n",
            "105/105 [==============================] - 0s 581us/step - loss: 0.7428 - acc: 0.6190 - val_loss: 1.5211 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.60000\n",
            "Epoch 55/201\n",
            "105/105 [==============================] - 0s 726us/step - loss: 0.7698 - acc: 0.6190 - val_loss: 1.4529 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.60000\n",
            "Epoch 56/201\n",
            "105/105 [==============================] - 0s 888us/step - loss: 0.7386 - acc: 0.6000 - val_loss: 1.4718 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00056: val_acc improved from 0.60000 to 0.62857, saving model to weights.best.hdf5\n",
            "Epoch 57/201\n",
            "105/105 [==============================] - 0s 632us/step - loss: 0.7267 - acc: 0.5524 - val_loss: 1.5169 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.62857\n",
            "Epoch 58/201\n",
            "105/105 [==============================] - 0s 558us/step - loss: 0.7376 - acc: 0.5524 - val_loss: 1.5606 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.62857\n",
            "Epoch 59/201\n",
            "105/105 [==============================] - 0s 612us/step - loss: 0.7619 - acc: 0.6190 - val_loss: 1.5929 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.62857\n",
            "Epoch 60/201\n",
            "105/105 [==============================] - 0s 612us/step - loss: 0.7256 - acc: 0.6762 - val_loss: 1.4261 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.62857\n",
            "Epoch 61/201\n",
            "105/105 [==============================] - 0s 696us/step - loss: 0.7022 - acc: 0.6190 - val_loss: 1.5452 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.62857\n",
            "Epoch 62/201\n",
            "105/105 [==============================] - 0s 627us/step - loss: 0.7320 - acc: 0.6286 - val_loss: 1.5707 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.62857\n",
            "Epoch 63/201\n",
            "105/105 [==============================] - 0s 701us/step - loss: 0.7111 - acc: 0.5810 - val_loss: 1.6587 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.62857\n",
            "Epoch 64/201\n",
            "105/105 [==============================] - 0s 506us/step - loss: 0.6578 - acc: 0.6571 - val_loss: 1.8652 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.62857\n",
            "Epoch 65/201\n",
            "105/105 [==============================] - 0s 514us/step - loss: 0.8444 - acc: 0.5333 - val_loss: 1.5183 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.62857\n",
            "Epoch 66/201\n",
            "105/105 [==============================] - 0s 591us/step - loss: 0.6821 - acc: 0.6381 - val_loss: 1.5318 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.62857\n",
            "Epoch 67/201\n",
            "105/105 [==============================] - 0s 605us/step - loss: 0.6442 - acc: 0.6000 - val_loss: 1.5442 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.62857\n",
            "Epoch 68/201\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.6998 - acc: 0.6190 - val_loss: 1.5543 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00068: val_acc improved from 0.62857 to 0.62857, saving model to weights.best.hdf5\n",
            "Epoch 69/201\n",
            "105/105 [==============================] - 0s 395us/step - loss: 0.7445 - acc: 0.6286 - val_loss: 1.5553 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00069: val_acc improved from 0.62857 to 0.65714, saving model to weights.best.hdf5\n",
            "Epoch 70/201\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.6914 - acc: 0.6571 - val_loss: 1.5765 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.65714\n",
            "Epoch 71/201\n",
            "105/105 [==============================] - 0s 730us/step - loss: 0.6922 - acc: 0.5810 - val_loss: 1.6197 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.65714\n",
            "Epoch 72/201\n",
            "105/105 [==============================] - 0s 500us/step - loss: 0.6919 - acc: 0.6095 - val_loss: 1.6406 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.65714\n",
            "Epoch 73/201\n",
            "105/105 [==============================] - 0s 425us/step - loss: 0.6320 - acc: 0.7143 - val_loss: 1.6881 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.65714\n",
            "Epoch 74/201\n",
            "105/105 [==============================] - 0s 520us/step - loss: 0.6437 - acc: 0.6857 - val_loss: 1.8223 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.65714\n",
            "Epoch 75/201\n",
            "105/105 [==============================] - 0s 562us/step - loss: 0.6642 - acc: 0.6667 - val_loss: 1.6697 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.65714\n",
            "Epoch 76/201\n",
            "105/105 [==============================] - 0s 640us/step - loss: 0.7409 - acc: 0.5143 - val_loss: 1.7459 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.65714\n",
            "Epoch 77/201\n",
            "105/105 [==============================] - 0s 535us/step - loss: 0.6855 - acc: 0.6095 - val_loss: 1.9460 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00077: val_acc improved from 0.65714 to 0.68571, saving model to weights.best.hdf5\n",
            "Epoch 78/201\n",
            "105/105 [==============================] - 0s 354us/step - loss: 0.6752 - acc: 0.6095 - val_loss: 1.7234 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.68571\n",
            "Epoch 79/201\n",
            "105/105 [==============================] - 0s 395us/step - loss: 0.6594 - acc: 0.6476 - val_loss: 1.7919 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.68571\n",
            "Epoch 80/201\n",
            "105/105 [==============================] - 0s 329us/step - loss: 0.6191 - acc: 0.6857 - val_loss: 1.7078 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.68571\n",
            "Epoch 81/201\n",
            "105/105 [==============================] - 0s 357us/step - loss: 0.7066 - acc: 0.6381 - val_loss: 1.9959 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.68571\n",
            "Epoch 82/201\n",
            "105/105 [==============================] - 0s 316us/step - loss: 0.6543 - acc: 0.6667 - val_loss: 1.6837 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.68571\n",
            "Epoch 83/201\n",
            "105/105 [==============================] - 0s 395us/step - loss: 0.6538 - acc: 0.6476 - val_loss: 1.7079 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.68571\n",
            "Epoch 84/201\n",
            "105/105 [==============================] - 0s 498us/step - loss: 0.7237 - acc: 0.6190 - val_loss: 1.7996 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.68571\n",
            "Epoch 85/201\n",
            "105/105 [==============================] - 0s 414us/step - loss: 0.7275 - acc: 0.6571 - val_loss: 1.7139 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.68571\n",
            "Epoch 86/201\n",
            "105/105 [==============================] - 0s 408us/step - loss: 0.6886 - acc: 0.6476 - val_loss: 1.8061 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.68571\n",
            "Epoch 87/201\n",
            "105/105 [==============================] - 0s 586us/step - loss: 0.6826 - acc: 0.6571 - val_loss: 1.7717 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.68571\n",
            "Epoch 88/201\n",
            "105/105 [==============================] - 0s 477us/step - loss: 0.7029 - acc: 0.6286 - val_loss: 2.2130 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.68571\n",
            "Epoch 89/201\n",
            "105/105 [==============================] - 0s 569us/step - loss: 0.7482 - acc: 0.6095 - val_loss: 1.9095 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.68571\n",
            "Epoch 90/201\n",
            "105/105 [==============================] - 0s 578us/step - loss: 0.6593 - acc: 0.6667 - val_loss: 2.0205 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.68571\n",
            "Epoch 91/201\n",
            "105/105 [==============================] - 0s 769us/step - loss: 0.6939 - acc: 0.6286 - val_loss: 2.0424 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.68571\n",
            "Epoch 92/201\n",
            "105/105 [==============================] - 0s 649us/step - loss: 0.6779 - acc: 0.6571 - val_loss: 1.8934 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.68571\n",
            "Epoch 93/201\n",
            "105/105 [==============================] - 0s 507us/step - loss: 0.6229 - acc: 0.6857 - val_loss: 1.9610 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.68571\n",
            "Epoch 94/201\n",
            "105/105 [==============================] - 0s 552us/step - loss: 0.6025 - acc: 0.7333 - val_loss: 1.9945 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.68571\n",
            "Epoch 95/201\n",
            "105/105 [==============================] - 0s 558us/step - loss: 0.6851 - acc: 0.6000 - val_loss: 1.9473 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.68571\n",
            "Epoch 96/201\n",
            "105/105 [==============================] - 0s 681us/step - loss: 0.6942 - acc: 0.6381 - val_loss: 2.0076 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.68571\n",
            "Epoch 97/201\n",
            "105/105 [==============================] - 0s 686us/step - loss: 0.6924 - acc: 0.6476 - val_loss: 2.0266 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.68571\n",
            "Epoch 98/201\n",
            "105/105 [==============================] - 0s 456us/step - loss: 0.6642 - acc: 0.6667 - val_loss: 2.1371 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.68571\n",
            "Epoch 99/201\n",
            "105/105 [==============================] - 0s 444us/step - loss: 0.5999 - acc: 0.7333 - val_loss: 2.1655 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.68571\n",
            "Epoch 100/201\n",
            "105/105 [==============================] - 0s 487us/step - loss: 0.6966 - acc: 0.6476 - val_loss: 2.3787 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.68571\n",
            "Epoch 101/201\n",
            "105/105 [==============================] - 0s 701us/step - loss: 0.6497 - acc: 0.7333 - val_loss: 2.0347 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00101: val_acc did not improve from 0.68571\n",
            "Epoch 102/201\n",
            "105/105 [==============================] - 0s 816us/step - loss: 0.6512 - acc: 0.6762 - val_loss: 2.0417 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 0.68571\n",
            "Epoch 103/201\n",
            "105/105 [==============================] - 0s 709us/step - loss: 0.6850 - acc: 0.6571 - val_loss: 2.1227 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00103: val_acc did not improve from 0.68571\n",
            "Epoch 104/201\n",
            "105/105 [==============================] - 0s 518us/step - loss: 0.6397 - acc: 0.6762 - val_loss: 2.0619 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00104: val_acc did not improve from 0.68571\n",
            "Epoch 105/201\n",
            "105/105 [==============================] - 0s 488us/step - loss: 0.6283 - acc: 0.6952 - val_loss: 2.0975 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00105: val_acc did not improve from 0.68571\n",
            "Epoch 106/201\n",
            "105/105 [==============================] - 0s 535us/step - loss: 0.6543 - acc: 0.6762 - val_loss: 2.1252 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00106: val_acc did not improve from 0.68571\n",
            "Epoch 107/201\n",
            "105/105 [==============================] - 0s 431us/step - loss: 0.6170 - acc: 0.7143 - val_loss: 2.1360 - val_acc: 0.6286\n",
            "\n",
            "Epoch 00107: val_acc did not improve from 0.68571\n",
            "Epoch 108/201\n",
            "105/105 [==============================] - 0s 659us/step - loss: 0.6220 - acc: 0.7238 - val_loss: 2.2249 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00108: val_acc did not improve from 0.68571\n",
            "Epoch 109/201\n",
            "105/105 [==============================] - 0s 694us/step - loss: 0.6728 - acc: 0.6571 - val_loss: 2.1202 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00109: val_acc did not improve from 0.68571\n",
            "Epoch 110/201\n",
            "105/105 [==============================] - 0s 864us/step - loss: 0.5908 - acc: 0.7333 - val_loss: 2.2894 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00110: val_acc did not improve from 0.68571\n",
            "Epoch 111/201\n",
            "105/105 [==============================] - 0s 741us/step - loss: 0.6584 - acc: 0.6667 - val_loss: 2.1305 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00111: val_acc did not improve from 0.68571\n",
            "Epoch 112/201\n",
            "105/105 [==============================] - 0s 870us/step - loss: 0.6471 - acc: 0.6762 - val_loss: 2.2831 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00112: val_acc did not improve from 0.68571\n",
            "Epoch 113/201\n",
            "105/105 [==============================] - 0s 693us/step - loss: 0.6813 - acc: 0.6762 - val_loss: 2.3273 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00113: val_acc did not improve from 0.68571\n",
            "Epoch 114/201\n",
            "105/105 [==============================] - 0s 414us/step - loss: 0.6869 - acc: 0.6381 - val_loss: 2.3418 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00114: val_acc did not improve from 0.68571\n",
            "Epoch 115/201\n",
            "105/105 [==============================] - 0s 551us/step - loss: 0.6712 - acc: 0.6667 - val_loss: 2.3432 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00115: val_acc did not improve from 0.68571\n",
            "Epoch 116/201\n",
            "105/105 [==============================] - 0s 772us/step - loss: 0.6587 - acc: 0.6667 - val_loss: 2.6740 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00116: val_acc did not improve from 0.68571\n",
            "Epoch 117/201\n",
            "105/105 [==============================] - 0s 605us/step - loss: 0.7453 - acc: 0.6667 - val_loss: 2.3434 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00117: val_acc did not improve from 0.68571\n",
            "Epoch 118/201\n",
            "105/105 [==============================] - 0s 437us/step - loss: 0.5940 - acc: 0.7238 - val_loss: 2.3975 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00118: val_acc did not improve from 0.68571\n",
            "Epoch 119/201\n",
            "105/105 [==============================] - 0s 540us/step - loss: 0.6499 - acc: 0.6857 - val_loss: 2.3466 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00119: val_acc did not improve from 0.68571\n",
            "Epoch 120/201\n",
            "105/105 [==============================] - 0s 485us/step - loss: 0.5916 - acc: 0.7333 - val_loss: 1.8856 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00120: val_acc did not improve from 0.68571\n",
            "Epoch 121/201\n",
            "105/105 [==============================] - 0s 619us/step - loss: 0.6434 - acc: 0.6762 - val_loss: 1.9405 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00121: val_acc did not improve from 0.68571\n",
            "Epoch 122/201\n",
            "105/105 [==============================] - 0s 610us/step - loss: 0.6120 - acc: 0.7143 - val_loss: 2.0248 - val_acc: 0.7143\n",
            "\n",
            "Epoch 00122: val_acc improved from 0.68571 to 0.71429, saving model to weights.best.hdf5\n",
            "Epoch 123/201\n",
            "105/105 [==============================] - 0s 450us/step - loss: 0.6862 - acc: 0.6571 - val_loss: 2.1574 - val_acc: 0.7143\n",
            "\n",
            "Epoch 00123: val_acc did not improve from 0.71429\n",
            "Epoch 124/201\n",
            "105/105 [==============================] - 0s 409us/step - loss: 0.6608 - acc: 0.6667 - val_loss: 2.1632 - val_acc: 0.7143\n",
            "\n",
            "Epoch 00124: val_acc did not improve from 0.71429\n",
            "Epoch 125/201\n",
            "105/105 [==============================] - 0s 448us/step - loss: 0.6303 - acc: 0.6857 - val_loss: 2.1953 - val_acc: 0.7143\n",
            "\n",
            "Epoch 00125: val_acc did not improve from 0.71429\n",
            "Epoch 126/201\n",
            "105/105 [==============================] - 0s 473us/step - loss: 0.5879 - acc: 0.7429 - val_loss: 2.2713 - val_acc: 0.7143\n",
            "\n",
            "Epoch 00126: val_acc did not improve from 0.71429\n",
            "Epoch 127/201\n",
            "105/105 [==============================] - 0s 675us/step - loss: 0.6697 - acc: 0.6571 - val_loss: 2.2938 - val_acc: 0.7143\n",
            "\n",
            "Epoch 00127: val_acc did not improve from 0.71429\n",
            "Epoch 128/201\n",
            "105/105 [==============================] - 0s 600us/step - loss: 0.6941 - acc: 0.6476 - val_loss: 2.6102 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00128: val_acc did not improve from 0.71429\n",
            "Epoch 129/201\n",
            "105/105 [==============================] - 0s 507us/step - loss: 0.6098 - acc: 0.7048 - val_loss: 2.0223 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00129: val_acc did not improve from 0.71429\n",
            "Epoch 130/201\n",
            "105/105 [==============================] - 0s 533us/step - loss: 0.6898 - acc: 0.6381 - val_loss: 2.1153 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00130: val_acc did not improve from 0.71429\n",
            "Epoch 131/201\n",
            "105/105 [==============================] - 0s 455us/step - loss: 0.6686 - acc: 0.6571 - val_loss: 2.1282 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00131: val_acc did not improve from 0.71429\n",
            "Epoch 132/201\n",
            "105/105 [==============================] - 0s 418us/step - loss: 0.6520 - acc: 0.6857 - val_loss: 2.1017 - val_acc: 0.7143\n",
            "\n",
            "Epoch 00132: val_acc did not improve from 0.71429\n",
            "Epoch 133/201\n",
            "105/105 [==============================] - 0s 437us/step - loss: 0.6171 - acc: 0.7238 - val_loss: 2.5948 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00133: val_acc did not improve from 0.71429\n",
            "Epoch 134/201\n",
            "105/105 [==============================] - 0s 586us/step - loss: 0.5970 - acc: 0.7238 - val_loss: 2.0644 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00134: val_acc did not improve from 0.71429\n",
            "Epoch 135/201\n",
            "105/105 [==============================] - 0s 437us/step - loss: 0.6086 - acc: 0.7143 - val_loss: 1.9982 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00135: val_acc did not improve from 0.71429\n",
            "Epoch 136/201\n",
            "105/105 [==============================] - 0s 394us/step - loss: 0.6403 - acc: 0.6857 - val_loss: 2.1318 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00136: val_acc did not improve from 0.71429\n",
            "Epoch 137/201\n",
            "105/105 [==============================] - 0s 468us/step - loss: 0.6070 - acc: 0.7333 - val_loss: 2.1832 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00137: val_acc did not improve from 0.71429\n",
            "Epoch 138/201\n",
            "105/105 [==============================] - 0s 402us/step - loss: 0.6151 - acc: 0.7048 - val_loss: 2.2002 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00138: val_acc did not improve from 0.71429\n",
            "Epoch 139/201\n",
            "105/105 [==============================] - 0s 466us/step - loss: 0.6555 - acc: 0.6667 - val_loss: 2.5713 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00139: val_acc did not improve from 0.71429\n",
            "Epoch 140/201\n",
            "105/105 [==============================] - 0s 518us/step - loss: 0.7505 - acc: 0.6190 - val_loss: 2.7662 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00140: val_acc did not improve from 0.71429\n",
            "Epoch 141/201\n",
            "105/105 [==============================] - 0s 498us/step - loss: 0.6412 - acc: 0.6762 - val_loss: 2.3688 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00141: val_acc did not improve from 0.71429\n",
            "Epoch 142/201\n",
            "105/105 [==============================] - 0s 394us/step - loss: 0.6406 - acc: 0.6857 - val_loss: 2.4248 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00142: val_acc did not improve from 0.71429\n",
            "Epoch 143/201\n",
            "105/105 [==============================] - 0s 482us/step - loss: 0.6442 - acc: 0.6762 - val_loss: 2.4780 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00143: val_acc did not improve from 0.71429\n",
            "Epoch 144/201\n",
            "105/105 [==============================] - 0s 449us/step - loss: 0.5904 - acc: 0.7238 - val_loss: 2.5033 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00144: val_acc did not improve from 0.71429\n",
            "Epoch 145/201\n",
            "105/105 [==============================] - 0s 438us/step - loss: 0.6665 - acc: 0.6571 - val_loss: 2.5286 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00145: val_acc did not improve from 0.71429\n",
            "Epoch 146/201\n",
            "105/105 [==============================] - 0s 481us/step - loss: 0.6150 - acc: 0.7048 - val_loss: 2.7474 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00146: val_acc did not improve from 0.71429\n",
            "Epoch 147/201\n",
            "105/105 [==============================] - 0s 506us/step - loss: 0.6993 - acc: 0.6476 - val_loss: 2.7100 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00147: val_acc did not improve from 0.71429\n",
            "Epoch 148/201\n",
            "105/105 [==============================] - 0s 526us/step - loss: 0.6446 - acc: 0.6762 - val_loss: 2.9709 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00148: val_acc did not improve from 0.71429\n",
            "Epoch 149/201\n",
            "105/105 [==============================] - 0s 748us/step - loss: 0.6473 - acc: 0.6762 - val_loss: 2.7400 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00149: val_acc did not improve from 0.71429\n",
            "Epoch 150/201\n",
            "105/105 [==============================] - 0s 934us/step - loss: 0.6208 - acc: 0.7048 - val_loss: 2.2753 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00150: val_acc did not improve from 0.71429\n",
            "Epoch 151/201\n",
            "105/105 [==============================] - 0s 633us/step - loss: 0.6041 - acc: 0.7143 - val_loss: 2.1731 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00151: val_acc did not improve from 0.71429\n",
            "Epoch 152/201\n",
            "105/105 [==============================] - 0s 685us/step - loss: 0.5948 - acc: 0.7333 - val_loss: 2.2308 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00152: val_acc did not improve from 0.71429\n",
            "Epoch 153/201\n",
            "105/105 [==============================] - 0s 527us/step - loss: 0.5691 - acc: 0.7429 - val_loss: 2.4344 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00153: val_acc did not improve from 0.71429\n",
            "Epoch 154/201\n",
            "105/105 [==============================] - 0s 563us/step - loss: 0.6256 - acc: 0.6952 - val_loss: 2.2793 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00154: val_acc did not improve from 0.71429\n",
            "Epoch 155/201\n",
            "105/105 [==============================] - 0s 477us/step - loss: 0.5821 - acc: 0.7333 - val_loss: 2.4207 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00155: val_acc did not improve from 0.71429\n",
            "Epoch 156/201\n",
            "105/105 [==============================] - 0s 439us/step - loss: 0.6453 - acc: 0.6762 - val_loss: 2.4627 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00156: val_acc did not improve from 0.71429\n",
            "Epoch 157/201\n",
            "105/105 [==============================] - 0s 500us/step - loss: 0.6017 - acc: 0.7143 - val_loss: 2.4986 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00157: val_acc did not improve from 0.71429\n",
            "Epoch 158/201\n",
            "105/105 [==============================] - 0s 469us/step - loss: 0.5679 - acc: 0.7429 - val_loss: 2.5336 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00158: val_acc did not improve from 0.71429\n",
            "Epoch 159/201\n",
            "105/105 [==============================] - 0s 456us/step - loss: 0.6595 - acc: 0.6667 - val_loss: 2.4971 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00159: val_acc did not improve from 0.71429\n",
            "Epoch 160/201\n",
            "105/105 [==============================] - 0s 538us/step - loss: 0.5860 - acc: 0.7238 - val_loss: 2.8811 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00160: val_acc did not improve from 0.71429\n",
            "Epoch 161/201\n",
            "105/105 [==============================] - 0s 492us/step - loss: 0.6226 - acc: 0.7048 - val_loss: 2.4589 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00161: val_acc did not improve from 0.71429\n",
            "Epoch 162/201\n",
            "105/105 [==============================] - 0s 669us/step - loss: 0.5874 - acc: 0.7333 - val_loss: 2.4120 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00162: val_acc did not improve from 0.71429\n",
            "Epoch 163/201\n",
            "105/105 [==============================] - 0s 649us/step - loss: 0.6285 - acc: 0.6857 - val_loss: 2.4269 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00163: val_acc did not improve from 0.71429\n",
            "Epoch 164/201\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.6514 - acc: 0.6952 - val_loss: 2.6267 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00164: val_acc did not improve from 0.71429\n",
            "Epoch 165/201\n",
            "105/105 [==============================] - 0s 796us/step - loss: 0.6599 - acc: 0.6667 - val_loss: 2.7295 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00165: val_acc did not improve from 0.71429\n",
            "Epoch 166/201\n",
            "105/105 [==============================] - 0s 605us/step - loss: 0.6362 - acc: 0.6952 - val_loss: 2.8349 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00166: val_acc did not improve from 0.71429\n",
            "Epoch 167/201\n",
            "105/105 [==============================] - 0s 680us/step - loss: 0.5759 - acc: 0.7524 - val_loss: 2.9486 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00167: val_acc did not improve from 0.71429\n",
            "Epoch 168/201\n",
            "105/105 [==============================] - 0s 715us/step - loss: 0.6026 - acc: 0.7143 - val_loss: 2.9843 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00168: val_acc did not improve from 0.71429\n",
            "Epoch 169/201\n",
            "105/105 [==============================] - 0s 431us/step - loss: 0.5737 - acc: 0.7524 - val_loss: 3.0487 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00169: val_acc did not improve from 0.71429\n",
            "Epoch 170/201\n",
            "105/105 [==============================] - 0s 353us/step - loss: 0.6072 - acc: 0.7048 - val_loss: 3.0582 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00170: val_acc did not improve from 0.71429\n",
            "Epoch 171/201\n",
            "105/105 [==============================] - 0s 383us/step - loss: 0.6078 - acc: 0.7048 - val_loss: 3.0854 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00171: val_acc did not improve from 0.71429\n",
            "Epoch 172/201\n",
            "105/105 [==============================] - 0s 354us/step - loss: 0.6418 - acc: 0.6952 - val_loss: 3.1334 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00172: val_acc did not improve from 0.71429\n",
            "Epoch 173/201\n",
            "105/105 [==============================] - 0s 380us/step - loss: 0.5823 - acc: 0.7333 - val_loss: 3.3202 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00173: val_acc did not improve from 0.71429\n",
            "Epoch 174/201\n",
            "105/105 [==============================] - 0s 459us/step - loss: 0.5484 - acc: 0.7524 - val_loss: 3.1346 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00174: val_acc did not improve from 0.71429\n",
            "Epoch 175/201\n",
            "105/105 [==============================] - 0s 432us/step - loss: 0.6696 - acc: 0.6762 - val_loss: 2.9366 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00175: val_acc did not improve from 0.71429\n",
            "Epoch 176/201\n",
            "105/105 [==============================] - 0s 398us/step - loss: 0.5486 - acc: 0.7619 - val_loss: 3.0217 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00176: val_acc did not improve from 0.71429\n",
            "Epoch 177/201\n",
            "105/105 [==============================] - 0s 427us/step - loss: 0.5907 - acc: 0.7333 - val_loss: 3.0813 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00177: val_acc did not improve from 0.71429\n",
            "Epoch 178/201\n",
            "105/105 [==============================] - 0s 495us/step - loss: 0.5970 - acc: 0.7143 - val_loss: 3.0979 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00178: val_acc did not improve from 0.71429\n",
            "Epoch 179/201\n",
            "105/105 [==============================] - 0s 485us/step - loss: 0.5435 - acc: 0.7714 - val_loss: 3.0939 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00179: val_acc did not improve from 0.71429\n",
            "Epoch 180/201\n",
            "105/105 [==============================] - 0s 371us/step - loss: 0.6075 - acc: 0.7143 - val_loss: 3.1832 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00180: val_acc did not improve from 0.71429\n",
            "Epoch 181/201\n",
            "105/105 [==============================] - 0s 623us/step - loss: 0.5401 - acc: 0.7619 - val_loss: 3.2984 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00181: val_acc did not improve from 0.71429\n",
            "Epoch 182/201\n",
            "105/105 [==============================] - 0s 421us/step - loss: 0.6000 - acc: 0.7143 - val_loss: 3.1952 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00182: val_acc did not improve from 0.71429\n",
            "Epoch 183/201\n",
            "105/105 [==============================] - 0s 468us/step - loss: 0.5440 - acc: 0.7714 - val_loss: 3.1027 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00183: val_acc did not improve from 0.71429\n",
            "Epoch 184/201\n",
            "105/105 [==============================] - 0s 411us/step - loss: 0.5487 - acc: 0.7524 - val_loss: 3.2367 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00184: val_acc did not improve from 0.71429\n",
            "Epoch 185/201\n",
            "105/105 [==============================] - 0s 433us/step - loss: 0.6469 - acc: 0.6857 - val_loss: 3.8693 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00185: val_acc did not improve from 0.71429\n",
            "Epoch 186/201\n",
            "105/105 [==============================] - 0s 448us/step - loss: 0.5876 - acc: 0.7238 - val_loss: 3.2698 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00186: val_acc did not improve from 0.71429\n",
            "Epoch 187/201\n",
            "105/105 [==============================] - 0s 501us/step - loss: 0.6166 - acc: 0.7048 - val_loss: 2.9235 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00187: val_acc did not improve from 0.71429\n",
            "Epoch 188/201\n",
            "105/105 [==============================] - 0s 596us/step - loss: 0.5663 - acc: 0.7429 - val_loss: 3.1346 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00188: val_acc did not improve from 0.71429\n",
            "Epoch 189/201\n",
            "105/105 [==============================] - 0s 415us/step - loss: 0.5640 - acc: 0.7524 - val_loss: 3.0714 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00189: val_acc did not improve from 0.71429\n",
            "Epoch 190/201\n",
            "105/105 [==============================] - 0s 460us/step - loss: 0.6239 - acc: 0.7238 - val_loss: 3.1656 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00190: val_acc did not improve from 0.71429\n",
            "Epoch 191/201\n",
            "105/105 [==============================] - 0s 455us/step - loss: 0.6744 - acc: 0.6571 - val_loss: 3.1296 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00191: val_acc did not improve from 0.71429\n",
            "Epoch 192/201\n",
            "105/105 [==============================] - 0s 436us/step - loss: 0.5202 - acc: 0.7810 - val_loss: 3.1212 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00192: val_acc did not improve from 0.71429\n",
            "Epoch 193/201\n",
            "105/105 [==============================] - 0s 451us/step - loss: 0.5563 - acc: 0.7429 - val_loss: 3.1462 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00193: val_acc did not improve from 0.71429\n",
            "Epoch 194/201\n",
            "105/105 [==============================] - 0s 439us/step - loss: 0.6236 - acc: 0.6952 - val_loss: 3.2209 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00194: val_acc did not improve from 0.71429\n",
            "Epoch 195/201\n",
            "105/105 [==============================] - 0s 475us/step - loss: 0.6618 - acc: 0.6857 - val_loss: 3.1515 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00195: val_acc did not improve from 0.71429\n",
            "Epoch 196/201\n",
            "105/105 [==============================] - 0s 587us/step - loss: 0.5898 - acc: 0.7333 - val_loss: 3.7191 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00196: val_acc did not improve from 0.71429\n",
            "Epoch 197/201\n",
            "105/105 [==============================] - 0s 807us/step - loss: 0.6207 - acc: 0.7143 - val_loss: 3.4343 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00197: val_acc did not improve from 0.71429\n",
            "Epoch 198/201\n",
            "105/105 [==============================] - 0s 556us/step - loss: 0.5517 - acc: 0.7524 - val_loss: 3.2646 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00198: val_acc did not improve from 0.71429\n",
            "Epoch 199/201\n",
            "105/105 [==============================] - 0s 537us/step - loss: 0.6261 - acc: 0.7048 - val_loss: 3.4679 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00199: val_acc did not improve from 0.71429\n",
            "Epoch 200/201\n",
            "105/105 [==============================] - 0s 640us/step - loss: 0.6051 - acc: 0.7048 - val_loss: 3.0334 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00200: val_acc did not improve from 0.71429\n",
            "Epoch 201/201\n",
            "105/105 [==============================] - 0s 572us/step - loss: 0.5503 - acc: 0.7619 - val_loss: 3.0339 - val_acc: 0.6571\n",
            "\n",
            "Epoch 00201: val_acc did not improve from 0.71429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yyem-2d6bjuh",
        "colab_type": "code",
        "outputId": "e44bbc53-7e36-4f1f-e74c-041dfd31e833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "cell_type": "code",
      "source": [
        "#Virtualize Training\n",
        "fig, ax = plt.subplots(2,1)\n",
        "ax[0].plot(history.history['acc'], color='b', label=\"Training accuracy\")\n",
        "ax[0].plot(history.history['val_acc'], color='r', label=\"validation accuracy\",axes =ax[0])\n",
        "legend = ax[0].legend(loc='best', shadow=True)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAFNCAYAAADLgfxRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXd8FHX6x98zW7KbCmlAIIUEEEzo\nTZqigKCIDVHwxIIKKHgWztPTO/VQ7GIX9ewNEYVT1AMRQamGThJ6gCSQ3nuyuzO/P76Z2d1UOtHf\nvF+vvDY79fud3Z3PPM/3+T6PpKqqioGBgYGBgUGrQj7XDTAwMDAwMDBoiCHQBgYGBgYGrRBDoA0M\nDAwMDFohhkAbGBgYGBi0QgyBNjAwMDAwaIUYAm1gYGBgYNAKMR/PRk8//TQ7d+5EkiQeeeQRevXq\npa/7/PPP+e6775BlmYSEBB599NEz1lgDAwMDA4P/L7RoQScmJpKWlsaiRYuYN28e8+bN09eVl5fz\n/vvv8/nnn7Nw4UJSU1PZsWPHGW2wgYGBgYHB/wdaFOiNGzcyevRoAOLi4igpKaG8vBwAi8WCxWKh\nsrISp9NJVVUVQUFBZ7bFBgYGBgYG/w9o0cWdn59PfHy8/j44OJi8vDz8/f3x8fFh1qxZjB49Gh8f\nH8aPH0/nzp2bPV5eXtmpt9qDtm19KSqqPK3HPFcYfWmd/Fn68mfpBxh9aa38WfpyJvoRFhZwwvsc\n1xi0J56ZQcvLy3nnnXdYvnw5/v7+3HLLLezdu5fu3bs3uX/btr6YzaYTbmhznEzHWytGX1onf5a+\n/Fn6AUZfWit/lr60hn60KNDh4eHk5+fr73NzcwkLCwMgNTWVyMhIgoODARgwYADJycnNCvSZeCo5\n3Vb5ucLoS+vkz9KXP0s/wOhLa+XP0pcz0Y+TEfwWx6CHDRvGihUrAEhJSSE8PBx/f38AOnbsSGpq\nKtXV1QAkJycTExNzwo0wMDAwMDAw8KZFC7pfv37Ex8czefJkJEni8ccfZ8mSJQQEBDBmzBhuv/12\nbr75ZkwmE3379mXAgAFno90GBgYGBgZ/ao5rDPpvf/ub13tPF/bkyZOZPHny6W2VgYGBgYHBCfDy\ny1by8yXmzas51005bRiZxAwMDAwM/vB88YWF996zUFFxrlty+jAE2sDAwMDgjFFYCC5X0+uLiqC2\n9tTPU1YGqiqxZ8+fR9b+PD0xMDAwMGhVpKVJ9Orlz3vvWRpdX1YGAwb48/DDPqd0HlWF0lIJgOTk\n0zuN91xywvOg/4y8/vrL7Nu3h8LCAqqrq4mI6EhgYBBPP/1Ci/v++OMy/Pz8ueiiixtd/+qrLzFp\n0mQiIjqe7mYbGBgYtGqSk03U1kps22YCHA3Wp6bKlJVJLF5s4bHHamjT5uTOU1UFTqcQ6KSkP4/d\naQg0cM899wNCbA8dSmX27PuOe9/LL5/Q7Pp7751zSm0zMDAwaO24XGIM+M47vZenp0t1r27R/O9/\nzQwc6KJjR5WMDLG8pkbiv/+1cOutDUX8eCgrk/T/U1JOzYI+dkxi2TK44gqQpJa3P5MYAt0M27Zt\n4csvP6OyspLZs+9n+/atrFmzCkVRGDJkGNOmTef999+hTZs2dO4cx5IlXyFJMmlphxk5chTTpk1n\n9uzpPPDA31m9ehUVFeWkp6dx7NhR/vrXOQwZMozPPvuIn3/+iYiIjphMcM01N9Cvn3uq2ubNv/Pe\ne29jsVgICAhg7txnsVgsvPLKi+zenYzJZOLBB/9BbGyXBsuKi4tZsuQrnnrqeQDGjx/FDz+sYvbs\n6cTGxgFw00238uSTjwHgdDr55z//TceOnVi+/Ae+/noRkiQxefJfKC0tJT8/jzvvvAuA++67m9mz\n76dLl65n+VMxMDBobaxZY2LOHBtFRfDXv7qXawKsCfW+fTLTp9uZMsXBq69W68sBvvzyVATa/f/u\n3TJOJ5hPUt1ef93KBx/A5s0S0dFqyzucQVqdQD/xhA/Llh1/s2QZFMWv2W0mTHDyxBMnF3qfmnqQ\nhQuXYLVa2b59K2+99R6yLHP99Vdxww03em27e3cKX3zxDYqiMGnSBKZNm+61Pjc3hxdffI1Nmzbw\n7bffEB+fwJIli1m48BsqKiqYMuVarrnmBq99ysrKePzxp4iI6MiTTz7G779vxMfHh9zcHN599yN2\n7NjGqlUrKSgoaLCsf/+BTfYrNjaOq6++jj17Urjttjvp128A33//LUuWLOb226fz0Ufv8fHHC6mt\ndTBv3uM88sjjzJ49nTvvvIvy8nJKS0sMcTYwMAAgM1MI8erVjQt0Xp5MVZUQaID9+2Wv9ZGRCtu2\nmdi3T+a885QTPr82/gxQXS2RmnpyxwE4fFi0KSTk3IozGEFiLdKlS1esVisANpuN2bOnc889Mygu\nLqa0tNRr2/PO647NZsPX17fRY/Xq1QcQ6VPLy8s5ejSD2Ng4fHxsBAeHeNXZ1mjTpg3PPfcUs2dP\nZ/v2rZSWlrB//1569uwNQJ8+/bjzzrsaXdYcPXokABAcHMLixV8ya9adfPXVF5SWlnDkyGGiomLw\n8bEREBDAs8/OJzAwiE6doti3by8bN67j4otHn8BVNDAwaI1s2GBi2jSbPjVp+3aZqVPtlJQ0v19a\nmsSUKXYOHRLCWFAgXhMT8Zrm5GkhHz0qk5oqJOfgQRlVdbu+779fhHEvWtS8caYo8NBDPrzyitVr\nuSbQoaFClJOTW5a27783M3u2DafTe3lGhkRoKNQlzDyntEILuuaErF2RM/XMTXyzWET0YXZ2FosW\nfc4HH3yOr68vU6de32Bbk6n5sQ/P9aqqoqogy+4vktTIgMczzzzJCy+8QkxMZ+bPfw4AWTahqt5P\nh40tq388p8c30WIRH/3777/D4MEXcPXV17F69c9s2LCu0WMBjBs3ntWrfyY7O4sZM2Y121cDA4PW\nz5dfWvj+ewtTpjgYM8bFZ59ZWLHCzG+/mZkwwdnkfm+9ZWXVKjMXXWRm5kwH+fniXuNwwObNJkaO\ndHkJMAjhO3hQvC8pkcjPl8jIkAgKUpk40cEDD9jYsaP5e+hrr1n58EMrvr4qs2bVUnd71seghwxx\nsWyZTHKyiYkTm24/wH/+Y2HjRjOzZtXSo4e436mqeJBISGj+up0tDAv6OCkuLqZt27b4+vqyb99e\nsrOzcThObrxEo0OHDhw6lIrT6aSoqIjk5OQG21RUlNOuXXvKysrYtm0rDoeDHj3OZ9u2LQDs37+X\nl156rtFlfn5+FBSIQicHDx6gsrJhoZLi4mI6duyEqqqsW/crDoeD6OgY0tPTqKyspKamhvvuuxtV\nVRkyZBg7d26jvLyMDh0iTqnvBgYni8slhODPSFVV83OGNRwOGlh+J0NGhvfUJO1VE9LGqK6GJUuE\nMubmiv01gQZYv14co6gIKiokZFm4itPS3BY0iAjujAyZyEgFu11Yv1lZTZ93wwYTzz4rLOfKSont\n293bahb0kCHi4rUUya0ojfc1N1eiulqitZSUaHUWdGula9du2O2+3HXXNHr27MNVV13LSy89R69e\nvU/6mMHBIYwZM44777yZ6OjO9OrVq4EVfu21k7jrrtuJjIziL3+5mQ8+eJcFCz4gOrozd999BwBz\n5jxMXFwX1q791WtZ586x2Gx2Zs6cRs+evWnfvqGoXnXVtbz88gu0bx/BddfdwPPPzyMpaSe33z6T\n++67G4AbbrgRSZKwWCxER3fmvPN6nHSfDQxOlTlzfEhMhHXrRAzKn4WcHImxY33p29fFhx9WN7vt\nbbfZyc6W+PnnU6sOqI0BJyWJwCotyUdzAr18uZmSEqmuzWI7TaBNJli/3gzU6sfu1Uthxw4T6emy\n13ETE01UVkpERgrrtUMHldRU4fqu70xUVfjrX21IEsyYUcs771jZsMHMoEHCNa6NNkZEqMTEKKSk\nNP/FSE+XdKvb86FBc8kbAt0KqT9lql+/AXpEtclkYv78N5rd3zP6+ocfVgHwxhvvAhAb20VfFxvb\nRV8eGRnFtGnTMZlMTJt2YwPL9I47ZnLHHTP195dddgXgnhrmSWPLXn75Tf3/WbPu9WoTwLBhIxg2\nbIT+/r///Z/+/6WXjvM6Vk1NDQcPHmD27IbnMTA4W2zcaObwYSgpgbZtz3VrTg9OJ8yYYSMzU24x\nq1ZZGaxaZUJRhCVtaTwHyHGd89gxtwV98KBMdbV4f+hQ0wL35ZfuE2oWdEGBhL+/SkKCxJYtMuXl\nbvf20KEuduwwsWOHTGmpRGioQn6+zOrVwhiJihIWdkSESlKSRGkpBAV5nzM7WyI9Xeayyxzce68Q\n6PXrTdx3n3ZNRDsCA1XOO09hxQozhYVQVwm5AZ7JTDwFWnuoaC0C/Sd6/vxjUlBQwPTptzBz5jQm\nTJhAeHi7c92kRklOTmL69FuZNGmyXm7U4Oxz7JjEunUtz/MsLobly93blZfDkiXCEjlVVqwwUVx8\n6sfR2LFDPu70jIoCR496ByadLqqqxBxd5eSCf0+JF14QFiFAfr6sB2nt3Cmze7f3tfn9dxMul4Sq\nSk1eg/T0lr8nmZkSLpfY/8gRmY0b3dtrQVz1ycqSWLPGRP/+LgIDVS8Xd2ioysiRImFIYqJJt0YH\nDHBhsagkJorjjxkj3NDa+6goccHbt1fq2tXwu6AFffXqpRAaqtKjh4vNm036w4ynQMfFKXofPDl8\n2H1NPIPIPLfTBLpz5yYu2lnGEOhzzNSpt/Lhh1/w7rsfMXPmzJZ3OEckJPTk448XMnbs5ee6Kf+v\n+ec/fbj+ejv1JhA0YMECKzff7KuPxX38sYWJE+G3304ticOuXTJTp/ry1lvWljc+DlQVJk+2c9dd\ntuPaPidHwuHQROH03r4++8zC9Ol2liw5u47FrCwR/BQVpTBlihhc1wRyyhQ706d7XxvhQhZoAlmf\n+++3cf319mYfpDQxMpmEEn/1lbCMQ0MVPYirPsuXm1EUiUmTHISHK+TlSaiqeFgKCVG55BKx3cqV\nZv340dEKnTqp+uc2eLCToCD3e83FHRGh1l2PhudNShLf24QEIe5Dh7rqxqHFcm0MOiDALdD1vQD/\n+IeNiRPtHDki6RZ0aKiiu9Wh9bm4DYE2MPgDsWePCadToqioeetRm8t59Kh4PXas5bHF4yEtTex/\n5MjpuXXk5koUFsocOtS4xVYfz6jgxgTkVNi7Vxz7t9/OrkB/+im4XBKzZtXSt68QoIMHZTIyJPLz\nxbXxDBzTgrCgcYGuqhJWttMpNft5awFiWmDV1q3iuOPHi+gzT9evhmZ5Dh7sIjxcpaBAWPFOp7Cg\nL74YwsIUliyx6OeOilJ0EQaIi1Pp0sX9PjJSfPAdOohljQWKaeft2VNsM2yYy+taaA+sAQHox67f\n9717ZVRVYtEiC0lJMu3bKwwY4KK4WKKw0DvjWXR0k5ftrGIItIHBH4TaWjH/FLxTGzZGZqZ3dK32\nqlk1x0tmpsSMGTY96EYTBO34J8Pzz1tZvFiIoHYTra6WmrQGPdFEBU5coH//3cQ119iZMMHOHXfY\nyMnx3l9ry4YNJ+ZlqKwUAUwnkwNaVeGjj8DHR+Waaxy6uBw6JOtWntMpkZ0t2lpaKrwYGlqQ1tdf\nm3nySSuqKoS2tlby6lNjaGJ02WXucPCYGIU+fUQbGhPopCQTVqtK164K4eEqqirpyUdCQxXMZpg0\nyUlRkcTatSaCglSCgoQVrREXpxAb636vubg7dBBCrX23nnzSqs+LTk42ERys6NtoDxVugW7exV1R\n4Xadf/yxhawsmZ49FWJjVa9tMzJkQkMV/JrPfXXWMATawOAPQlqarI8Zlpc3L07Z2eKnrY1Raq+e\nAtcSDgfccYedpUstLF0qbpR5eZLX8U+Uigp48UUfXnxRVC/yHv9ruW2eFvSJjkF/8omF9evNJCaa\n+O47C3fdZfOyTLW2pKfLXgk2WmLdOhNffmnhiy9OPFpr2zaZPXuESLZpg5e4eAq+9mC1aZMJRZHo\n1080XHuoeeMNK6+/7kNSkuw19tyYyGpo1/Lii53YbEKoEhJcTY7halHe552nYLVCu3ZiH+3hTcu8\nNXmycNOrqjtCW7OS27RRCQlxW9Bt26oEBIjjay7u7GyJnByJ11/34V//spGfL3HkiEx8vKJHd4eE\niGhtLXahrEzCZlOxWiEsTCUgQPVycXv+rw2NJCS49Hakpkp6fIPW1taAIdAGBpye4KkzTWqqWzQ8\ncw/XR1Hc43inYkE/9ZQPW7aIm73mdtQEIStLOqlgKs3lfviwTFlZ/Skudf+7XEgFBY3+FR0oIIR8\nQsin+mjBCRUSTk6W8fVVycwsZ9w4B+vWmXnxRTGWXloq0lFqnIgVrQlZY2OnLbFwoRB1TdTat1fx\n81M5eFD2miqkPTBo48/XXCO2z80VY8Da57pwocWr7c0JdEaGhCSpREWpeqKOnj0VL9Gq38+aGome\nPcXDQViY+NFoQWyhoeJ99+6K7qp3C7R4jYsTIqudw9P1rbm4MzNl3aVdXCwxf75Vb5snHTuKaPCa\nGmFBBwSI82vHP3zYPTSgXYcJE9wT6BMSPPsqk5srUVsr6RZ9a8AQ6BPkuusmUFlZyaeffkRy8i6v\ndZWVlVx3XfPVrdasEdOvfvxxGb/+uvqMtdPg+ElNlYiO9ufHH1v3rENPi6Y5Czo/3zOQyvv1eC3D\nHTtkFiyw6q5IzcWamyva4HA0HkTUEp5WckqKqdEpLhnxEwnt0bnRv7e/6Ug+YeQTxiufdyT4gr4t\nirT9PwsI7hpF6b4c4uMVTCZ47bVqoqIU5s+3kpLiTqAxbJhw965bd/zfBW3fE/UqZGVJLF1qoWNH\nuOgioSSSJETs8GGZXbvcQqtdm40bhYtZc0vn5kqUlLiHPL75xsK2bSZ69XLh76+2INAyHToIq1ML\nvkpIcBESohIU1HBfTTQTEsR3IjxcvO7ZI9rpmbtae+DQplBpoqcJomalewq0v78I8srMlLymQX34\noUVvmyeauzsnR6KsDAID3evi4hRqaiT9+6b9dm680UHnzop+PO37ffCgrMdXeLbpXHNc38Knn36a\nnTt3IkkSjzzyiJ4zOicnh7/97W/6dhkZGcyZM4cJE5oXqT8DU6feesL7ZGVl8vPPKxg5clSLZSoN\nzh67dpmorpZYssTM5ZefhvRMZwjPG2ZzY9CamIIQZkVBD4IpLBRzVFuaKaeN7T38cA2PPOKjj995\njhNnZ0uEh5+Y68HTRe0pjGKdRG1mAf0KfyGDToSN798gYcUvv5hwOCRqHdDfdw+dj+7GvG0rzguG\nNHlOnyWLMZUUM4YVyD3/AkCbNvCPf9Rw1112Vq0y69bbFVc42bNHZsMGU6MJMxpD68OJjMtr857L\nyiSef14k+NCIi1PYtctEZqZEdLRCWppMerqYH52SIsZOO3ZUkWUxzUm7pmazSnGxaMOwYS4kSQRG\nuVzexwcxfJGZKTFokBC9u+6qJTBQPChoFuiuXd5VodyR1JpAi89eC67TLGiAG25wkJYmc9NN4uGp\nXz+F+++v4aqrxO+rRw+Fhx6q4eKLvX9vEREK2dluCzoyUtEfTupb0BERbou7rEyiUyfvcW4Qn01M\njEsX6C5dFF58sZqtW03ExIj2BgaqdVnNtKjy1uNOa1GgExMTSUtLY9GiRaSmpvLII4+waNEiANq1\na8enn34KiDzPU6dO5RItzv4PxLRpf+Hpp1+iffv2ZGdn8cgjD/L662/z73//k6qqKqqrq7n//gc5\n/3x3gtZ5855g5MhR9OnTl0cf/Tu1tbV6MQyAn376H19/vQiTSSYmJo6HHnqU+fOfY8+eFD788D8o\nikKbNm2YOPEG3nrrVZKSdiJJcOWVExk3bjyzZ09n4MDBbNu2heLiYp577mXat2+vHz83N+e4ykSO\nGnVpo8u00pMA//zn37n22uvZvn0rmZnHyMrK5JVX3uKZZ+aSl5dLVVUV06ZNZ9iwEXoaUVmWSEjo\nzfjxV/L88/N46633APj44/fx9fXj7rvrFYZtxWgR0SdyUz4XeFrQzbm4PYWioECiuBh97BqE5aS5\nNJtCs2B69XLRvr2qR1l7CnRmpkQj9V2axVOgt20zkZYmER/vIiVFZJrKX7KBjsDbzOSGp+/VrSQQ\nKTCvivKnVy+F1FQTU/2+5rXKSVg3rMV5wRDy8oTF1K+fu29SeRnmHdsBGMkaChOm6OuGD3cHGvXt\nK5Z17aowZIiLH36wkJ5+fOUGtc8lN1fSBW3TJhPdu7to06bxfZ591sqmTWauvNLBjBkW8vPd6zRx\nARg3zsk771jJyJDYv1/G4ZBISHBhMglBzM2V9Ws6ebKDzz6z1vXNSU6OxM6dJo4ebdiPY8ckFMU9\n3tqli8rjj7trIMTFKWzdamLBAitxcQpjxzp10YyPF9dNE+iqKq1Qhfscvr541VQwmeAf/3B7OiQJ\n5sxp6Plo315l3z6JzZtFgNns2bU89JANm031ui7gtqDT0kR6Ts3FLfrjFuhRo1ykpsr4+Kh06qQS\nHe1ixAiX17ZJSbLuQftDubg3btzI6NGiclFcXBwlJSWUl5c32G7p0qWMHTsWv1MMf/N74p8E9084\n7j9iYlrcxu+JfzZ7zgsvvJj1638DYO3aXxk58hIKCgq44oqref31d5g5czaff/5xo/uuWPE/YmPj\neOut9+jatZu+vKqqipdeep0FCz4gPf0IqakHmTJlKn369OO229zitWPHNg4dSmXBgg/4+OOP+eCD\nd6msFMU//Pz8ePXVBVxwwVB+++0Xr/MWFORz22138vrr7zB+/JUsWbKYysoKPvroPd58813mz3+D\nlSuXN7qsOZxOB2+99R4VFeUMGnQBb7zxLnPnPsP7778DwCuvvMiDDz7CggUfUFhYgM1mw+GoJTc3\nB4ANG9YxatSYZs/R2tDSFubny3oZvNbI8VrQnoke8vOlBvOFjycYKyVFxt9fJSZGJSJCpbJSCL2n\nQDeXN7kptHNLkspPP5lxuaS65BPCUlJ+WQvAGkZ6iTkIi93hEGOEYWGwoupCACzrxT5PPOHDhAm+\nXsFjlt83ItUNRF7MahLi3RZbu3Yq3bq5+P13k24FdumiMHSo2H7TppbHocvK3G5/RRGR6AcPSlx5\npS8vveTT6D65uRKvv24lJkbh5ZerGzwQek5BGjDARbt2CunpciMuZpWcHLcbd9QoFwMGuLDbVS64\nwOUlUvXxLPPYGJoIP/mkD7feamfuXB9SUmRiYhQ9qKu+98RToE8WLVAsM1MmIcHFNdc48PVV6dPH\n1aC+sybQ2m82MNB9fs9AN1UV16BzZ6WBJ0Hrq8Mh8f33Fq99WwMtWtD5+fnEx8fr74ODg8nLy2uQ\nTWrx4sV88MEHp7+FZ4ELL7yYN954hYkTr2fdul+ZM+dhgoND+Pjj91i48FMcDgc2W+OJFI4cOUSf\nPv0B6Nu3v748MDCQf/xjDgBpaYcpKWk8Y8Devbvp06cfAL6+vsTExJKRkQFA797isT48PJySevXf\ngoNDeOWVF3n//XcoKyvlvPN6eJWJ9PGx8eyz89m9O7nBsubo0UN81gEBgezZk8J33y1BkmRKS8X5\n09PT9DrQ//rXXAAuvfRyfvllJaNHj8XPz5/g4JBmz9Ha8JxTvG6d6aTryJ5JSkrEA0R4uEJurtzs\nGLTm4jaZxDxVbay4Sxc4eFCzYpuuyFBVBQcOyAwY4EKW3cE7u3eLubXBwQqFhfJJBUVlZMjYbCId\n486d4m4ZF6ewd68IigorXEsFvmxmIOnpLgYP9t4XhKjk5sLG1DAc5ydg2fw71NSwf78vDocQLG08\n1LJ+HQAF5nZEOTMw+x4C3Gmihg518dFHJlatMuPrq9K+vUrv3uLaJCebuOGG5oc86otfZqakPyA1\n9bC3caMJVZW46aZaXew88RTonj1dREWpbN8u65WetLHY8HCV5GRJHwOOilL48MMq8vMlr/nAqaky\nl1zi/Xlrot6UtXjLLQ46dlSpqYH5831YsEBY5sOGuYOsQkKEm11RxLGCg09doLXvmuinQps2sGJF\npZd1rKG5uPftE/33FGhtnFkL/iovl5oU3kcfrWHYMBculxB9zfXdGjjhqBi1kXDX7du3Exsbe1wp\nINu29cVsbubJ9M1Xxd8J0NJzrm/dX1OEhfXhqacKcDrLqa6upH//BN544w2iojrx2muvkJSUxPPP\nP09YWAAmk0xoqD82m4WgIDs2m4U2bXwJCwtAVaswmWSCgnx45ZUX+PbbbwkLC2PGjBm0aSNa4ONj\nISwsAD8/H/z9bTidZmprawkLE79UWVYJCfHHajUTGhpIWFgA/v42HI4qfRuA+fOfZtSokUyZMoXl\ny5ezZs0aQkICsFhkr+0aWwaiFKX7nNCmjS9+fj60bRtAWFgAS5cupba2iq++WkRxcTHXXXcdYWEB\nmM2mBseaPHki99xzD2Fhbbn22qv09fW3a63UeFQ33brVxsMPN3wYO9d9OXRIvA4eLLNsGTgcVsLC\nGs/mVVgoXrt3l0hJgbw88d0bNEgIdH6+jbAw7z5mZ8Ojj8ITT0BOjnAnDxxoJiwsgC51aeQPHhTH\nGThQZsUKKCz0ISyscSuxKY4eFUkgBg40sXOnWNavnw/79kHG9jw61abwE2NwYKWwEMLC3PtqWbHi\n431ITxcWq3PYJVh2JxN2aDdHjwqLuqLCz73f7+tRLRZe5gGe4iEi9m2G4W6//OWXi3nINTUSffpA\nu3YBXHSRcMHu29f0NdbIzRWvXbvCgQPi3FlZYllmprnR783WreJ1/Hj39fPcbtAg8ernBwMH+tO1\nK2zeDL/8YkWW4aKL/PD1dSfT2LFDWH59+/p55SYfUFca4NgxG1u22Fi5El54QbibCwrEup497V7X\n2JPbbxevw4fD4MGiitXgwRbCwtzTydq1E5nQgoKgU6dT/913czshGTpUXP+m2qeVhDx4UMhYeLj7\n8woLg8hIsS4vT+hSr17ebdcIC4Pzzmt4/HP9m4fjEOjw8HDyPQZIcnNzCat3xdasWcOQIU0HaXhS\nVHRq1VfqI+pBNzMgd5wMGjSUZ555ngsuGE5eXhmZmTnExXUlL6+Mb7/9gcrKavLyynC5FPLzy6mu\ndlBSUkVYWASJidvo128oK1cTHBLGAAAgAElEQVT+isulkJ6egyTJgI3k5APs2pVEfn4pVqtVP05F\nRQ0WSzVdupzHxx+/zzXXTMHXV+bw4SP4+YVQW+ukqKiCvLwyysurqaio8epndnYeI0aEkptbyo8/\nLsflUggMDOfgwVTS0nIwmUw89ND9PPPMSw2WaQU0MjLyAEhOTqG4uFJvU15eGRkZ2bRtG0ZBQQXf\nfruM6mpx/qioGNas2Uh8fALPPDOXKVOmEhPTGbvdj6+/XsJLL71GXl7ZaftczgZZWTbAQlCQyurV\nKjk5FV5VklpDX7ZsMQN2zj+/hmXLfMjPd5CX13jFo8OH7YCZ7t0dpKRY2LKlFrAycCB88QXs29dw\n37lzffjgAyuSpNXGtREXV01enoOgIHHuDRscgIXu3WtYscKHw4ed5OVVHXcfysshPz+Anj2ddOni\nBMRDQlhYBe3ambmIXwHYYLkIHLB3by15ee6np+RkK+BDmzaVhIeLh4UjnYfQg9coWrqCggIh0Hv3\ninZLZaWEbN1KWcIglu4az1M8RPXylZRd6a7lHh8vAeIGHh3tvi6xsX5s3y6Rm1vebEzC9u2iTUOG\n1HLggJV9+6rrPAMW0tLURvdftcoXPz+ZqKhy8vIa/35deKGd9u1VCgqqCQ8X5zhyBLp2dVFRUUlF\nBQQGiuV794roZ4dDHE9DiHUAP/3k4qOPZCorJcaMqWDAAIXERPEdCQ0tJy+veYuxQwd48UUzDz9s\nY/DgSvLy3JZoaKgvWVkiiUheXsUp/1YCAkxo5lRUVIXXueojSWA2+3P4MICE2VxDXp57XHvgQBtL\nlli47z4XYKJDhyry8o4vCPRM/OZPRvBbHEQaNmwYK1asACAlJYXw8PAGlnJSUhLdu3c/4ZO3Ji66\n6GI9whpg3LjxLFr0OfffP4v4+AQKCgr44YfvGuw3btx4UlKSuPfeu8jISEOSJIKC2jBw4GDuuONm\nPvzwP9x441Ree20+0dGd2bdvL6+99pK+f+/efTjvvO7MmnUn06ZNY+bM2djt9hbbq5WJnDPnr4wa\nNZYdO7Z5lYm8554ZTJhwNXa7vcEySZK4+urrmD79Fp5++t+Nlo8cOfISNmxYy7333oXdbic8PJwP\nP/wP9977N95442Xuuut2AgICiYnpXLf9KMLCwvH1bSUpeE6A4mIxH3T0aCcFBbKeGak1oblS+/QR\nrsrmxqCzsiRCQhQ9qlUbX01IALtdbTAX2uFAz+y1ZImFbdvciRwAOoQ7MeMgebuCGQfREbW0C64h\nL9Mpdj7Ov6OHXZhxENOxhp7dqzHjwCrVEtOxhuiIWi5BxFlUDhLV1eqPQWvt1sagAY5EDUeVJKTV\nv2HGgRkHOUdFuyzr1yEpCqmdLmQ351PhFybGqz28gGFhKt27i356ukB79nRRUiKRcURptC9qrXg9\nckD06cIhoj/ZGS7SU8UyZ7WT3GPe1yjnqJPDBxSGDqzGQtPX6uuFpbzxchk4xPXW+tb7/Bp9m/Yh\n7uWdO9Ug4S20fn7CDXzwoCjrCLBhgxmXS7jZY2IUryC85rj+eiepqeX07u0tmNo49OkYfwYRJAZg\ntap069b8UJPJJLZXVXcWMU/mzq0hPFzRp6u1prHl40VSG/NZ1+PFF19ky5YtSJLE448/zu7duwkI\nCGDMGBEMNGHCBD788ENCQ0NbPOGZeCo519bN6eKP3Jennnqcyy+foJfc/CP15aKLfMnMlJk7t5p7\n77Xz5JPVzJjhHmsLMztx9elH5X1zqL7pFqSyUtqMu4SyG6cx7Ku/cf01VTz6/Qhqx0+g8v4Hz0gb\np02z8f33FnbuLGfAAD969VL43/8aeqNUFTp39qdzZ4UbbnDw2GM22rcXU1d27oRJk1zk5cns2+cO\n9FyxwsTUqb74+opgMKtVRVHg0KFyfDNTCbrkQsyVZ+ezrMCXlx/L5vW3/bDbYfPmCn3dlVfa2bTJ\nTHp6GUuXBnDvvfD++1XcPH8w5pSkJo/55jU/MnvpZWQMmUinjUso2LQdJTZOX//ww8J78OabVUya\nJCys116z8stTW/jNPAqTs6apQ7cqnPE9KVq11qtI9sSJdtauNXPttQ6WLLEwcqSTRx+tYcwYP266\nqZb580+tb/fea2PhQgvjxjn45JPqU/7dFxRI9OjhT69eruOqdX355b56Mp1XXqnixhu9LeQNG0xc\ne60dRZHYu7esyfKT9WktFvRxjUF7znUGGljLy5YtO+ETG/w5qKmp4Z57ZtCjx/le9bD/SBQVSbRp\no3LJJS5MJpVvvrF4CTQpKZjSj2D78nOqb7oFy7q1mA/sx/ndSvbseYhkcwaWpB2o/v5wBgRaVWHz\nZhNhYQrt24s0ho1MpABERqzKSomICFW3arQEGuHhYo7n/v0isYMWoKRls3rxxWruvttOba1Ejx4u\nbDawbEnEXFlGMvFk0QGAvn1dpKfJFBRKDBvmormQEk+OZYriDT16KISHqWQclfCxCiusshI2bzGx\njAmM7G0iMlJl5073HN6cHFHCsG9f0S7Ngs7Pl6h48B8UPvU+Bw8KS6pNG5XevYS1pERE8MXBi5Bl\nFZ9xw2HjEqzr11LtIdDTp9dSVSUxbpz75p6Q4CKExZicNTgGDEKt8wzV1sLWrTK1DglJAgmw+6r0\n76ewdq0Jm13Vpx0B9OiueEU7798vkZUt07evi8C662+1mqitbT5oL3GzuMg9e7oIrhtnLimBHXWB\ndn39DxCakoRpz25c8e7poHffXUu3bgpPPFFDSopMYqKJNWvEbV+LVj8VtGQlWlaxUyUkROXBB2vo\n1ev42iYCxcQ1aCzgbuhQF6++Ws2hQ/Jxi3NronWnTjJo9fj4+PDuux+dkWMXF4tiAGc6qrqkRKJb\nN4V27YRIr1xpZs8ej7nCdZFA5u1boaICS92UPDkjHQD1SJp4n5/X8OCngdRUiZwcmauvdiBJIslI\nUy5ubepThw6KV2YngJAQ97Sa9HSR2zg/X+Knn8ycf76LiROdvPeei23bTPpUHlNdH/9pe5Fvq8cB\nsP2Dcl592conn1j57ZkKKiuF+1mWYcQIZ5Nzf59/zIe3D1r53/wK+vdX0BI/lSAKTlwaI+6wsxPK\niIoS83CzsyU6dlT56isLiiJxww3iwSk8XOybny9Re9sVPLdxIu8cFAFCscEKm74WlndFBWzuZqd3\nbwXTqBHwuJiWVe2RaCg2VuXVV73H5BMSFLqzmhrZRumS78Em8nZPmmRnnUNYpEuXmlFViStHOXjv\nvWqm9vLTH4Z69HCxZ4+JRyfWcO+9tWzZInPsmMyTT/pQ6C+x/4dySuruvmFhAZQ0Y63V1MDYKH9U\nVSLly3JMdWKYmipx6RAx3PjtmP9w5dLpWNf/RpWHQI8a5WLUKHeJxg8/NPHee+KBTKsIdSpoDx/1\nv2unwoMPHn/6Vk8XfX0Xt0ZLkfitmdY32GZgUMdjj9kYM8aXoqIzd47qapFooU0b70T/X37pEe1Z\nF3kjORxYNv+OtW7qTkBhOhIKIWVCxOQCj2wTpxEt/7J2Qw0IUJsUaC1JSYcOqte4YNu2KhaLqFYE\n7gIH33xjxumUmDJFiL9Wj1grxqA9hFS3j9KPFRqq6jfGN9+0Mm6cH3feaef228V82aZoLlOTr694\neOjcWaFtW/f0n4wMMY/1yy/N+PioXHutaJ9mQdcvAtKhg0J2tqQPM2/eLDKPDRvmxNW1G0pYeINx\n6MZoZ86nN7vYYhkCdVMsV682sW6dmXHjHCxYUM399wsh6d7duxoTwMiR4vqlp4tpX1dc4cudd9pJ\nT5cZOrThnN7m8PGBmBi1buzdfQ6tWAVA7VAxbq9NK2sMLTFLTo5MbOzxjz83h/Z9OlfZtzynZTUl\n0H9kDIE2aLXs2SNTXS2dttrDjaElKdEEeuxYJ8HBCosXm3FoXm5tLg3g88MyTLuTAbAqNbQnm84c\nBkAqLMSrPNJpQku7qeWJ9vcXLu7GNEazoCMiFC+BDgkRN7KxY8UxFi8WDyBffmnBbFaZOFEsnzrV\nwUcfVXHTTaLzmgWtRAqBbttWxcfHPQd10SIL/v4qTz1Vja+vqtcUbgxtDnRT7tDPPqvi009FVLh2\nw09Lk9i2TebAAZNe8Qm8LWjt2Ha7Ss+eCpWVIj+197VzgSRRO2w4ppxsTIcONtlOAMuG9QAsr7lY\nf0DUMoZNmuREkoSl9+mnlUyfLoTaUyxGjnTq7Vq3TlSguu46B889V82zzzYefd8cH39cxWefeUfM\n+/mBr6+4TsH9InFFRWPZuI6mqphoJRrB/V06VUaPdvHZZ5W6Z+NsoyU2AUOgDQzOKppVdKI1jE8E\nLUmJJtBWK1x7rZP8fJlVq+rExmPuim3hp0iqiuojLMUYjhDDEQAkVeXTV0r59dcTqyfsyc8/m1i4\n0G1eqaoQmXbtFOLiRBsDAkQpv4qKhvtryUM6dFC93I6aWMfFqQwc6OK330wsX24iJcXEmDFOfb0s\nw+WXO6nrHqa0NFzt2hPSUSzQxhw9ra+XX65m+nQH55+vcOCATHW1aPcLL1iZOdOm/x04IBMZqTQ5\nbalHD0WP3NVc8RkZsl7GUfNugHDXg6cFLY7tmZ8ZhPfBZFIZPFiIk0OzNNetbbwRdVg2uDOaaWlP\nte+hVtvYZIKxY116kQZPsfDMjqYV3pg9u5bbbnPQqdOJC0n37opuqWtIknvsNypKwTF0OHJxMaaU\n5EaPERqq0qOHuA6nw72tteHSS11Ym58ufsbQor6h5fzyf0QMgTZolZSXi8IOgF5l5kygFRdo29b9\nQx8/XlgXm+sCczQL2tWxE1Jd5aS8YaLYycDQQ7pAA3zwXCkPPGA7qVKMINJV3n+/jcq6ANYDB2Ty\n8mS9+AGgZ1VqLJuYlp4yLk7U7NWsCk+xnjLFgapK/PWvdv19ozidyJlHUSKjdOHTxhx79FDw9VWZ\nMaNWL4CQkODC6RQ5o/ftk3nhBR+WLLHof1VVEgMHHp8waC7uZcvMfP65hagoRa/4BGCxiM8sP1+i\ntFR8jlFRqldN4fJyUZWrTx9Fv3k7htelB93QvEBb16/FabGTyCB9mprbRd/4h6uJRZs2KsHBKpGR\nKhkZEuvXmwgJaSiwp4NBg1wkJLgICoLaYSPq2v5bk9tfeaWTNm1ULrzw9Ht6zgXa9xIMC9rA4Kzh\nOQf2eHJHnyxadqqgIPePW0tZWFFRd946C7rmiqsAUG02tna5DoBLu3kLdBh5ulvzRFFV0W8xJUSz\nAD1ctHX4+4v21R+HPnpUYu1aE4MGOXUXsWYZe7q7r7rKgd0uKh+Fhip6EFF95OwsJKcTV1SUbjFr\nFlu7dir79pXz5JPuaTpataGkJJPe7rlzq9m1q1z/e/nl45vW07GjOM+ePSZkGRYsqGqQRzkkRKGg\nwF3NKTJSoX17twWdmChSk3q6c11xXXCFt8O6rulxaCk/H/Oe3ZT2HEwtPrprOy1N5CdvKghOEwut\n5nFkpEJtrUj9OWSIyyv5zenizTerWbVKPM056gTasqHpcegHHqhl9+7y0zZv+VyjPRRZLCpNZGP+\nQ2NEcRu0SjxFuSkX9w8/mHnpJSv//W+lVy3Y+pg3bSTolsmoldWUOHw58uY3RE4U+c8bs6D9/OpZ\nqLm5KIFB1F48Ct933sQx8AI2l3bnMqC3dQ8dyNb3DUdY2wsXWrjwQheBt/4FZ7fzuG7v0/j6wltv\nVeN0wm232WnbVuWNN9zjkXl5oioPiDzQ/fopDcafwT2dpH5Fq6++sqCqEpMnu7cNCVE5dMjbgg4I\nEF6Cr7+2cN11TiwNsx8C7vFnV1SMPr7qOWXIp148mJbYJDlZJidH9GPsWKeXG/J4sdnQ52//8581\nDBzY0PoMDRVlApOS3AKtWdBZWRKHDolr5zWdSJJwDB+BbcnXhEa3a7x0WV0cgXTxCNjmLrigudGb\nctFrDzFaQgzPPNeny6XcGFp7lE6RuKJj8Fn+o+jbOaLlbBinlwoAB9hjTuNBIyKQVqxBDQw6jQc9\ncQyBNmiVeIpyUxb0ypUmkpNN7Nlj0scYG8O2dDFyURE5QV1oV3OQ/Z8thXoCHeTxO9QKsuljvHl5\nKCEhOIaNoPr6KVRPvJ61r4oMau33CWulhECCKCXGN5fY9go//GCm7FgpYT8uQ0pOZnn6C4BwDRcU\nSKxcaUaWVZ55xi24nv1MThaisGGDiQ4dFDp39hxra+jiFpHOFux2lauucrusQ0PFPNH6gVn33VdL\neTnMmNH0lBY5XUwfUyKjGDrUxbXXOpg0qelgoO7dFUwmlV27TKSmSnTsqJxS4YFZs2rJzJS5667G\nzzl6tItNm8w88ogwnaKi3NHlx45J/PqrGX9/UdnJk6rbpmPKyABn031R/fxRplxPxEKFQ4dkiovF\n9Y6Karo//fu7uO46BzffLK6pZ2TzmRRoTyoe+Dv2j947K+dqDIvZhNN5dt3nxXkiat9ygrXJm8MS\nFYnqc+5NckOgDVol2rizxaLqU23qWy5amb/maiODmPeq+vpxVUQiv5a0p+N+9xhdcxZ0RYUkImLz\n81H7xYCPD2VvvIOqwtaZ/uTL4YRmHQVgCwMYxS/07ZiNeZKDp5/2Ye1nWcQC5syjyLhQMPHMM1a9\n+o+iSGzaZGLMGG1KjvuhJCnJxL59Mvn5MhMnOrz6ro1Be7q4N20yceSIzKRJDq+EDZors/481W7d\nFD75pPloYlOdQLsio/D3h7ffbn57u13UU96yRUZVJSZNcjSbx7olvJLFNMKsWbX8+quJtWvFbSwy\nUtEt/R9/tFBcLCpG+darlOMcfAHFP6w8rjbExSmsXWvWKyY1VyvYZhMeEg1t29BQ5axVSKuZchM1\nU246K+dqjLCwAIrPcgZBzQHUeL3AkyMsLABaQSZEYwzaoFWiWZP9+rmoqpLIy2t4p9dqE5eWNq0C\nUm4u5v37qOx3AYl727CJC+iYtwOprvxn/ShuEK5bk0mlokJCKi4ClwslxO24O3ZMoqhIoiAwWl+W\niChB1C04j+uvdyBJKrt/EGVDZaeDCDKZPr0WSRL5sJ98UtzItQhf0Wf3z3HPHpm1a4UoaPNXNdwC\nLd67XPDiiyKMtn7Al+by9QymOV70KVZRUS1s6SY+XtFzIw8ffmYTRJhMsGBBNeHhCpKkEh0tgsEC\nA1X9wetUk1Ro7uo1a8Rn0VSAWGPExopthw93ndKDisH/XwyBNmiVZGTIdYXatek2JyfQ1rpo3b3t\nL0JVJdYwEhkVy8YNQMN50CAsdT8/4eKW6yq5KR4V3JKTxc+mpkOMvkwT6ChbLhERYj6u40C6vj6G\nI0ybVsvSpVV8910lU6c6sFhUNmxwRz6lp4u29OrlorJS4rPPhG0wdKi3yGgWsubifuklK2vXmhk7\n1tnAlXrHHbW8915Vo2O4LSFnpKNKEq6Okce9jzYOLdp95l2d4eEq335byccfV+mpHDUrOjZWYdCg\nU2uDVlNZS495Igk5OndW+eSTSubO/WPk8jZofRgCbXDKLFli1qf3nC60gBwtU1H9QDGXC92q1gQ6\nO1vijTcsOD30TMus9JPzYgBWc3HdciHcjVnQINzcFRWSnh3M04LW5sWau7iFayv9UZAIrBER30OH\nuujkStPXn2c9TEyMGA/t3VvB11eMWSYlyXpSDa2Pl18uOrBnj6nRcVzPKO5160y89JKVyEiF116r\namCpBQWJqTUnY8GZMtJR2ndoGA3WDFokd2SkQnT02YkUjotTGTfOLcTaOPTkyafmYhfHFv3Zvt1d\nSetEGDfOdVJBcgYGYAi0wSlSUwOzZtmYM+f4b+Itoc1rjYxUvXJHe1JYKOFyibuv5ur95BMLc+fa\nWLnS7Ta2rP8Nxc+fL/cNwGZTyYgYRDU+ukCXlIjqTfUrfAqBBqkuv7bqUalNixoO6i1cv6rFQnVI\nBOU+IbqgDx/u9Jp+NTDscINpNkOHuvRxaBBegpAQxSvgbejQhu5Rt0DDe++JyO0FC6rq6v+eJpxO\n5GNH9Qxix0vv3i7atFH1ueTngr59XQQGqqclu5Um0Jrb/kQF2sDgVDAE2uCUyM4WQnnggEmfWnOq\neM5r1VyKmvtXQ3Nvg9uC1rJKaQIq52RjPniAyv5D2LXHxsCBLsIirWxkCOaUJKSiQr2SVX0R9PMT\nVaF0F7eHBZ2SYiI0VMEvXljQrk6RJG6pwh4dqhfMuOACl54CFCDe9zD10caW160zoyia10AlPt7l\nsU1DofN0cScni7acjAu7OeSsTCSXC9cJCnRQEGzfXs5jj507t+5DD9WyY0f5ack1HRmpYrWK4wQG\nql7R/gYGZxpDoA1OCS33M4gpQaoKd99tw7NC6Zw5PsycaWuQptr3mbmExHchJL4LgX+ZpCeO0Fy9\nQqDdLu7337cwfrwvlZWNC7QWGKSNEWtW8q62FwHCGm3XTmUNI5FUleAL+rImLZZLbCLvspx5jLbD\nBhAS34UVe2IYWfmjnqRECRVj0CUl4gEiIUFBiY4R66Ki8fMTVrZcVAROJ4GBEGc6QiqxAESpRxpc\nu/79XcyX5zBi8QPk5UnU1EhERiq0aQPPBc1jIZMZOkQItP2t1wlO6EpIfBf63jcWMw6OHZP1qlTa\nA4Z12X8J7t1dv64h8V1oO3wgck52g/MDmHftIHhAL6/tQ+K70Ha0SHrhio5udL/m8PPjhIpBnG5k\n+fSlfTSZoHNn7/SjBgZnC2OalcEpoeV+Bli3zkRcnMLXX1uQZbj1VpEv+tNP68oAxir8/e91825d\nLuzv/weptgbVZsNn5QpMBw/g6tpNDwiLjlbx94fgYIUtW0z8+qsoOpCcLHsJtDbdyC3QwmWsjT8/\ntno0VqvKlVc6KSiw8AU3Mqfbt/jWFBFVdIRJVZ8CfbD+uAzzgf24OkTQrvYot/M+ao6wnDWB1o6d\nkODC1TmW6onXUzv2Mq9tpIICsPkQ6CpmLUOxU0VoeRr1U2fblQpmq6/jLDTx7s/PA2IuL4rCPbXz\nsVNIofIQLrpg/+Bd5KJC1MBA/LetZxCJJCYOrWuLWzjsn3yIKSsTZ5euoi0VFZj378P6vx8g4b4G\nn5/P119hSj+CKyoG1eqdsUSJ6ETtuPFNfPL/f4iLU9i3z2QItMFZxxBog1NCK28IsGGDWU+aryiw\neLFZF08fH5WXXrISHa0QF6dwXvkOwkpLqLrpFpx9+hHwt3uxrF+Lq2s3Lxc3CNHascNtqaemivzU\nGqWl4lUT6KNHZYqKoO36tZTLAawp7c/Tz9XQtatCeLjKQbryv7nr6N+7hnY9ohhYKeZFa2Uki79b\njuvCCVxU9StqtsjdrI1Ba9Z5z54KmEyULXAnhVDrKjjIBfl6RaHDdKYtxQzJT6TC6fQyLS2bf8ei\nOrDg4LMXiwBfIiMVTHt2Y68qFNusX4dqtmBKT6Pm8glUX3cDQdNuYiRr2FAyrK4tda6J2losm3/H\n2b0HRb/9DoAp9QDBQ/rX5Z5uKNCWDetQfXwoXJfInzJX4mlAi+Q+W0FvBgYahovb4JTQCtS3a6eQ\nmirz5ZcWQkJEoYYvv7To5QgXLarCbIZ77rFz+eV+fHr7JgAcQ4fjGDYccLuktfFmreqPFsl9+eUi\n6OfgQdlrXnR9FzdA6tpczIcOska5kCuvUbn1VrGvVo0pJ0eiuNzCWkYQWbEPOTsLy8Z1uDpFokRF\ns6/DhYRQiO8WIdpKsBDfpCTNgm5oTWkWtJyfpyf5SJejKQyMRnI6kbMyvbb3zJlsPiamZEVFKV7F\nDizr1+rbOYYNxzFEiPJoebW+jdYW8/ZtSJWVOIYO19e5YrvgatdePHzUyz0tFRdhTtqJo/9AQ5yb\nQauwpX0PDQzOFoZAG5wSmgV97bVirLSiQuL6651cfTXs328iM1Pm6qsdDB3qYvHiKu65p4Zhw5z0\nL/sVEAn+XbFdcLXvgHW9KGCQlSXqBmvZr/7+9xpeeaWK554TgUcHD7pd3BaLqs8H9hToqv8Jsf9N\nvoh582r0MVotl3RurkxxsaRPu7K//y5yYaEoOCBJpEaJcWtLUT60bYuWsDo5WczP1pJQeKIFkskF\n+ZgyhEBfeU8E51/eEXAn/tCwrnMLsRbxHRmp6q551dcPy/q1+na1Q0eghoTg7BHPYGUDVmqw21U9\n0lib81077EL3SSQJx7ARyHm5sHev1/ktmzYiqaqXoBs05OqrnTzzTLVXuUsDg7OBIdBNsGyZmX37\nTv/l2b5dZsmS0zeyUFoKH3xgaRCA1RKKAh99ZNEL3qsqfPGF2ctlrbF0qZlDhxqP0M7KkjGbvfM/\nT5ni4Lbb3NtoN7ahQ13861+1zPt3BRfyG0d9u6B0iBAiMnS4sDz37yMrS6J9e3dkdZcuKjfe6CQ8\nXCUwUOXQIbdAR0crlJZKKIqoTCVyT4NrlRC5miEXelXucQu0RHGxSFwCYP/P24C7ZN/ROLdoqeHh\n/Oc/FubOtbJ/v0yPHkqDykrgTmYi5+ch14lx/PhOhA0UkdBabmsAyssx79iGWncgXaA7OoUlHxVN\nzaVjMeVk4/P9dyjBwbh6nC/aOHwEvlQxiETOP9/dFq3GcX3B1aocsXq113JLnaWulWA0aByrFW6/\n3aHnaDcwOFsclwI9/fTT3HDDDUyePJldu3Z5rcvKymLKlClcd911PPbYY2ekkWebnByJ22+38+9/\nn765vRoPPGBj5kz7aRPpjz6y8vDDNlatOrFEIb/8YuLvf7fx1lvCMkxMNHHffXZee8278vqxYxIz\nZtiZN6/xa6GJae/eChERCkOGOOneXWHMGBH9Gh/vajAFqJdrO4GU8b+qkbrQaiIi/7aWvDyp0dSU\nkiTGAw8flsnKkgkJUQgOVikrEw8qqirRt6+oU9yn+FeKCWLQ9PO9jtGunVug09NlttOXGp8ApEoR\nwqWJW02HGA4TA0ClbxiPPmrjjTd8cDqlJgtzqHUWtFSQjym9rhJUZLQ+VcnTgrZs/h3J6aR29KUA\nDAw5RLduLgIOJyMXFwl8mZEAACAASURBVFM7bASOOktYqqzAMWQ42kRqx1BxrUayxj0lq6YGy5bf\ncfaI18fCNbQhBNas8VpuWS/Gnx39BjTaHwMDg3NLiwKdmJhIWloaixYtYt68ecybN89r/bPPPsu0\nadP4+uuvMZlMZGZmNnGkPw4HDojLsn//6bWgCwvFHFoQQq2d51Q4fFgI3LFjJ3asnTtNXq/a3OH6\nfdamPDV2LVwuMQ+6fXsVkwlWr67g88+rADE9ZcWKCr79trLBHGPN0vtFvYSvvxYPKprlqv6yFlWV\nmpzDGhsrauweOiTXWdTgcomauyBqOV/cNZ2uHGSTdQQXj/bePyRERZJUcnMlNmww4cJMeT8xruuK\nikaJEtOK/PxU3f1d7hcOwM031/LTTxU8+mjjc3z1Mei8fEzpaSh+/qjBwbjqjmnysKCtdePt1Tf8\nBYBxPVL54YdK3U0txuZH6NvXDnNbxY6hw1CQuJjVeuYuy/atSFVVXttpuDrH4eoQIQS6bhxaKirE\nnJKEY8AgY/zZwKCV0qIZt3HjRkaPFne5uLg4SkpKKC8vx9/fH0VR2Lp1K/Pnzwfg8ccfP7OtPUuk\npoqbfUaGRHX16bt/bdggLveAAS62bDExY4aNVasaClhzJCbKvPyyD2+/XUVQkDuph+e0I4Cjaw7h\nvO2vPBL8NlmB3XjttWr9Zg7uaOSUFFEpSgt+0vquoU2jOnxYxuXCy7Wbny/hcsELWVNpO2QzXoms\nTDKdXY0H1ci5ombyevNF7Fxk4e67HSidY3F1iCDw1x/Zy3m0WaXSdkhDkX6rSOIpRJvsR8CUoVKO\nRPvJKnuRCFquYnWIh4TyASMa1Do2m4VI5+TIHDkiXOLWS4fDxuVermE/PzFfehofUuYjhPf88xU9\nN3hjaGPQPksWI1VX4ep2HkgSSsdOqJKEz7L/Yt6SKC5PVhaq2UztyEtQQkOxZqYTFOQOlHMMG4HS\nsROudu0x5WTr1jSA2jaYwwE9GVK2EWe3CsDqsV8j7uq6IQTTN18RPLgPqsmEVFMjxp89HgIMDAxa\nFy0KdH5+PvHx8fr74OBg8vLy8Pf3p7CwED8/P5555hlSUlIYMGAAc+bMafZ4bdv6Yjaf3rzNYWEB\nLW90AmhOAFWVKCkJIPL4awU0y/bt4nX+fBP//jesWmXCYgnwStHYUl9WroRVq2DXrgCuvdbd1rIy\nH8LC3G7o1HcWc0HFb1zm+Jg5Gc/yyy9+XHKJ+zh79ojXwkKZmpoAPX4oK0vGbg/QEz1oU5hqayUq\nKwOIjXUf48gRiCWVkccWgq8vBAZ6tbXJL5efL4y/nG55EaxaBTZbgMiOde9fqXr2FQKLS/FXwFx/\n4jDgr4CWW8sOSIpwA5krIRDwc4GPHY4o5zH4uRsavZ4REZCUJIzJSZMg4Pab4cdvsd1zN7a67SMi\n4Huu4FjchezpdhWsgpgYG2FhzTythfrDZZchb98OAf6Yp95Ud/4AuPFGWLUKc0W52DYwAK65mbDO\nHaBzZ+SdOwlra4dNGyA2lpC+da75v82BjRsJHj4Qz1yh+UMvwb5iFyPtyVjDLoLEDSBJBE0YCyGN\nfIfumg6bN2GqrnIv69IFv9um4neafz9ni9P9uz+XGH1pfbSGfpzwQKjqMVVDVVVycnK4+eab6dix\nI9OnT2fNmjWMHDmyyf2LiipPqqFNERYWQN5prtuZlGRHuzSJiVW0a3d68gr//LMvdrtM587ltG1r\nAywcOlSuF4E/nr6kpYn9tm2rYejQWtLT/QGJ9HQHeXnuWrTBu34B4NaYX5izH37/3Ulenrg5l5bC\noUPuL9/KlVWkpNigzjL9/fcKevUSluLBgz6AGJdOTKwkIMA9/rp7t5mLEYFHZf+aS/Xt0/V1x9OX\nmH/UAlZ+/72C3r0VmHY3C2ru5fHHbXz4alWj+ZyTk2UuuURE68yaVossq7z+ug8zptTyzjtWnnm0\nWgT01G3fWBuCg+2oqubNqCbP1gZ+WFW3g9je6TRRRDBv3fATPnXFIszmSvLyWojG+3iR93vt/C8v\naHz7vDICOnTCtnkzJYuWEFRcTNX4KynX9rtlhvgr8H5aibhxMKwAx8oVlHQ/n9CNG3Gdn0CRYm28\njm2vQf/X3t3HRVnm+wP/3DPDg8CoYDMoAkmoaYOapG6GD0lQVmunBxM8omaWdfJhbe2Ysq24W5A9\nuL+zqedV66utXXUN83A67ll/0bbl/spAzVoMzBRcyUeY4flBBGau3x+39zDIyAAyzD3D5/169cp5\n/t7XMPOZ+7qv+7pgKC11/p6oYN3b7nLH595TuC3q447t6EnguzxwaTQaYbk6HzEAlJeXw3B1tGpo\naCgiIiIQHR0NrVaLqVOn4tSpU90uQm0cu3lPn+6d49AWi4Tvv9di0iQrAgLkeX2Btlmwukrpyi4p\n0eDSJQktLdLV6x3qbGjALZYjAIDQkm8wamiNvUsbaDsOfuutctj893/r0NwsISBA2J9b4Tiq21n3\n9904AAA96ipVTg9yfF5l6lBlycBrOZ7eZDTa7PNSK+dOX7sqlTPKSG4AHZZnVCgjdhsaAOXPPzTU\nPRNVKMe9A3fvAtBxFLYzLVPvgpAk+Tzpb76G1NTk9PgzEXkvl+mTkJCA3NxcAEBRURGMRiNCrvZ/\n6nQ6REVF4cyZM/bbY2Ji3FdtH2hulr/sDQZlD7J3AjovTw5FZYGEGw3o4mJNuyUYHReq8DtyCDrR\niiYpEJLViseHfYFLl9om91AGhCmr/eTmynuTiYmt9udWKIOvrr0eAC5ekEcSXxl0E6y3junWdgBt\nAe34vMox74gI52EYFARERsqPMxoF9HplMQ35OboSouHh8uMNBhtGjXL+QyA4WH6ehgYJFRXydWFh\n7gloZZS3/yf/F0DXfuyIwaFojRsPv6NH4P/Zp1cfx9OliHyJy/SJj4+HyWRCamoqXnnlFWRkZCAn\nJwd//etfAQDp6elYv349UlNTodfrkeh4oFMlbDZ5KcL/+A9/bN3q1+mqS2fOaGC1Srj7bit0OtFr\nAX3woBzQyiL2yp6fcoz3zBkJ//VfHSZ7Qnm5hD//ue1IRFmZXM/p01K7FZ7KyyX7Y8Xn8oChfZHP\nAgDu0R4A0DYwTJlPOjnZiiFDbPa98IcflgPacY/20iXJfm6xcv3HH2vxww8a2E6eRiTOo2GyPLlH\ndylTKLbfg5ag1QoYDNcPQ2UvOjxc2H/oKAE9aFDX96ATEjou5ahwFtDu24OWA1pqaUFrzC2wRQzv\n0uNaEqZDunIFge+/CyFJaJl6l1vqIyLP6NIx6BcclyYCMGZM297SzTffjN27d/duVb3s0CEtXnih\nbXBPeXkzfv1r56fKKGFx6602jBhh65Uu7n/+U8KHH/pBrxeYOLH9HrQyTeWmTQHIyQF+8xs/pKW1\nTfrx+uv++OMf/XHgQANGjLDZ97grKzX2U6SCggQaGyVUV8uTXkkHvkQrtDg04+d4fM82jKs4AEAO\n5lmzrCgs1GDAAIGRI22Ii7Ph73/XQJIEkpNbERjY9qNEOY0qPt4Gf395Ks+SEgmLFgXhppts+Ded\n/ENAkzgd3ZwnBQAwfLhAYKDo0MUdHi6cTgSimDTJirw8LWJibGhokB+rtEtXQlSZujE5+fpjC67t\n4h4wQCAoyOVT94g1qm3FqO5MGtKSMB14eys0tTVoGTcBYnBvLghNRJ7WL2YSU/Y0n3xSXkmps/Ob\nlXAaOdKGkSNtqKqS7OsM90RTE/D00wNQVychK6vJvpjEtQGtdD+vXx/Q7nixEsLyAhHt6zhwQL4t\nPl6Ox/JyDVBfD/0PR3EEk2EcZ0DrxDtwU+m30KMWRUUaNDcDP/zQNhuWMo9zbKwNISHy5CIlJfKp\nV2azvNZzRIS8wMWFCxq8+668ARaLBmMuydN12mb07FQdjab969ls8h60q3V8V69uxsGDDRg+XFw7\ncLxL6/XefbcVX3zRgLlzOwvo9nvQ7tp7BtBuzeXuTLvZcudUCPvkJTz+TORr+sVqVsqCDklJrfjf\n/9XZQ/jKFeCFFwIxb14Lpk+XQ66kRIIGVty/eylmfnsK5ZAw9CEbgkPkL+gLFzQICGibJ/p6yso0\nqK6WJ9HYfkU+9zb69zbg9/Ltj9VKuB0aRPxWYHC2DdtOaHAZEnAFCLgfGDTGCkjA9mNaCADDf2lD\ncAhwCBpotfLeLa6Oxws/KVAGCSOX2DBQ1witrRUHcDdGj7Shedp0+B3OxyHNVLT8JQghhQJftmgw\npFRg8H02bKiUsAgahFrkyx9d0KC6QULIPTYENMuvZ/xKwCYACyRI7wLPaoCQEIGo2iJYdOEQo0b3\n+L0ZOdKG77/X4tIlCVot0NoqXXeAmCIwEBgxQm5/5Ri0oitd3JIk95B0ZsAAQJIEGhqAigrYR9q7\nxYABsBmM0JjLuzXYTgwajNZxE+BX8C2n6yTyQf0ioJWRyMOGyQsL5Odr0dQEHD2qRXa2H65cgUNA\naxAvfQtj7m5YNToY4Qe/M/IEF0IAhib5C15nvv7rCQGENQFhVy9rNIB/AyA5rFUw2AbEQX4eXTUw\nsgmAJN/XegXQfC+/julqLmjLAK1FfoxGAI7x4lcFGAD4lQJaHVDtb8Ce5nl4L9aGK0MewYA/voeY\nyn/CdgWQTsnP4V8HaE8ANwlALwF+jfLlES3yecb+J+WTq+IA+FXLrzMUAASg1QB+zUCzBiiYvATj\ne3D8WeE4klsJ2+sNEHNG6YlQ/q3rpb9ojUYekFZdLaGuzn0DxBRNj86FpuwSbEOHde9xi5YAOxym\n8yQin9EvuriVkcHDhsnd1kJIOHNGY+9KdhwNXVKiwSOD5XN7C1a/g2A0Ys2zVbCUluGrv5oRjEYE\niUaUHCuDpdT5f5t+WYlgNOLNX1WiobwMdZfKUPFj+/sc/ER+rmfSqmEpLYMxuAFTxzfiw/ctCEYj\n1q2swtubKxCMRgSjEXMSa/Gbl9ueV7n+7sl1+NN2+X5Zv6iEpbQM00dfxMmg2zFsmIDVFIeK46ex\namm1vfalqTWoOifXUfFjGerLyuyXf/d/5Of6j8xK++v84T8rkLPDYn/Nz//XjIof5e0a/z/pN/Te\nOI7kVkaMDx3a9WX9HAO6K6dYdUdIiLD/bbg7oBte3oS6373f7cc1LXwC1Z/8HSLE85MqEFHv6icB\nLS9fGBraNgK4uFhjH82sHKOurpaPrSZeHfUc/OA0+30BeepPxfHj1x/F9MEHftDpRKfHOB2PQbe0\nAPX1EsLCgDvvtEKjEfjqK619+k3ltZXR5yaTzb63GRVls69xXF6ugc0mn7t9yy02x4mnEBcn9xCM\nGWPFpk1tE5pcSxlZ/f33GnvPw9ChbUsajh5tRXx8762L6ziS29UpVs4oM54BvR/QwcHyMWjAvceg\niYic6Tdd3MOGCftqSIAcYsr5wGazBpcvyyGhRSsm1H6J1ltiERo3zL68IdB2Kg8gn0s8dWrHscuF\nhRp8950Ws2e3dHqqkON50DU1SgjIg5zGjbPh6FEtmpvl60eMsOHHH9uWWAwPlwPzH//QIjra1m4J\nxYsXJTQ2SvZAVcyZ04oTJ5qxZElzp6OR4+JsCA0V+POfdfZJPCIibIiMFMjIaMIdd9h6ckbVdTl2\ncQcGdr+LW6drG8XelePP3aEMFAPcvwdNRHQtn9+Dbm6WA1gZeKQE9PHjmnajuc+e1aC4WIOJ+BYD\nmmvRkjCj3fKGVmv7gFb2vq/1wQfy6gzz53c+PWhwsDwIqbYWqKmRrwu7etD6rrusaG6WcPSofCrR\n2LFWNDbKM5EB8gxaSrBFRYl2Aa2csnRtQIeEAL/61RX74KrrCQgAHnusBWazxj6BSXi4/ONm+fIW\n3HlnT06our7Bg+UFK44d0+DgQfn1utPFDbT92OntvVwGNBF5ks8HtNItrJy6Ex0toNMJfPqpDq2t\nEjQa+fqzZ+Vwa5u6Uu7eVpY3/PFHyd7FrdEI+963ox9+0GDnTj/cdJMNSUmdB7RGI09WUlsroapK\nfl4loKdNa3vsuHFW+wji776TR5APHAhMmCAH5dixVgQGyqOXy8vlUAdcj1LuTGqqfB725cvyjGr+\n/i4ecINMJhvKyzX4+mstgoKEy9OsrqUEtDu6uBXs4iaivubzXdzKwKOIiKvr5voBN9/cNjnGT35i\nRV6eDj/+qEFJiQbPXDO3tGOX+Nmzcjfs2LE2FBbK5xQr4dXQADz1VCAaGyVs2dLUYZlDZwYOFKir\nk1Bd3dbFrdSk0QjYbBLi4mwICpLDoblZQlSU3MX85JMt+MlPrJg4UZn20oayMg327PHDgAEC99zT\n8wU+xo2z4bbbrDh+XNvtsOyJrVubcOiQ/MPilltsCAhw8YBrKLOy9X5Atz2fq9PqiIh6m08FdHU1\n8MtfBuLf//2Kfa/z0iUJwajHk3kroS1aBqspDmNjGpFR8m8YhouILbOhBBpEbrFheqWE2/EVWkeO\ngi18KIC2gC4u1uDHHyVERtowbpwV334rT3f56ac6fPmlFmazhB9+0OLpp5sxZ07XwlGvFzh/XmMP\naGUPWt5DtuHbb7WIi7Pap+IE2qap9PeHPZyV60+dkvfGH3uspcMEHt0hSfJe9IYNWvsPG3cKDxd4\n6KGe/6DgHjQR+SKfCuhPPtEhO9sPQ4YIbNwoT+V54YKEh/ERbv/6fTS93Yy6LW/jAb9PsBA75Qed\nBmIA4Fzb8zTMTbH/Wxn1XVCgRWWlBhMmtNpn3/rFLwKQn9/WhLNmtSIjw/kUos4MHChw4gTsXdyO\n60KnpragulrC5MlWnDvX1p2ujNi+luMKTUoX9Y14/PFWvP++DTNn9u4xZ3doC+jefV7HPWgGNBH1\nNZ8KaGWU81dftQ3gunCh7biy38EvACEwuVGeonJxZC5ez5+MmJgQDBwoYKnQ4JGHW/CfP2/bm1MC\nWplWMyrKZj9lKT9fhyFDbPjrXxsxdKg8f3R3RjgPHAgIIeH8eeVc27bblixpwZIlctBKUvs9ZWeU\n6yMjbfZJV27EkCEC+fkNru+oAu7bg2YXNxF5jk8NElNWejp2TGNfJerSJQmzIE88oj13FpofSzHy\n7N/RhABcuWMqNP46DIvSoqzCH1boEDOqfZMoyxtaLPL10dECt91mg0YjIEkC27Y1ITJSnsGqu6cf\nKecyK4PPQq+z1sHAgW3h4yqg581raXf+c3+gHIPu/VHc8v91uvbnWxMR9QWf2oNWFpOw2STk52tx\n771WWP95DrE4DaHVQrJaEfCXPyP4n8dwath0LFulAWBDdHTbqlXXnp4EyHvRSjdzdLQNQUHAhg1X\nMGgQkJjY873Va9cydtyDvlZUlA3V1drrBvSjj7bgxx8lPP30jXdve5sFC1pgtQJTpvRud7yyBz1k\nSI9W0yQiuiE+ta+ldHEDwJdfyr89Ys/K3dlNqQsAAEHbfgtJCAxPS8C4ccq5xG2hrAwKc+R4nXLf\n555rwYIFNxaG165l7CqggevvQQ8fLvDGG1f6ZVfsqFE2vPzylS6NnO8Ox4AmIuprPhfQer2Av788\nVabNBtxefTWgn3watiFDoDGXA0C7VYMcVypSjjk7ah/QvReAykjrqioJWq2wd9U6M2aMXMPNN7t/\nVDXJlC7um27ybB1E1D/5WEBrEBlpwx13WPHdd/LMYDPFAdT5haLVNA4td8mhLAIC0BI/yf44Ze90\n6FCb02ONSmgHBopOp+/sLselEgcPFp12o65Y0Yz/+Z9G3HYbA7qvcA+aiDzJZwK6qUleGtBgEEiM\nr8C74klceeBfEYMzKImYBmg0aL66qH3L5J/IiwpfpQS0s+5tx+uVSUJ6i+NKTIMGdX7fkBA4nfub\n3Ef5scaAJiJP8JmAVgaIGY0CaX7ZWIL3kVi7DwBQdc/DAIDme2fDNnAQrjwyt91jx4yxYdgwG2bN\nch6AkZECY8daMWNG7wakY0DzPFv1iY21YcgQG6ZxqWUi8oAujeLOyspCQUEBJElCeno6xo8fb78t\nMTERQ4cOhVYrnyf85ptvIjw83D3VdkIZIGY0Cowo/X8AgNI9n8E2ejTiIuSDvbaoaFQUn+3wWL0e\nKCi4/jm/Gg3w97839nrNjrN99fZKTHTjDAaB779vgMGgh9ns6WqIqL9xGdCHDx9GaWkpsrOzUVJS\ngvT0dGRnZ7e7z/bt2xHsOC+iB5SXy50BRoMV/h9+AasxHEEz71D1+THXHoMmIiJSuOzizsvLQ1JS\nEgAgNjYWNTU1qK+vd3th3aXsQY8WP0BjLkfLtOmqDmegfUCzi5uIiBy53IO2WCwwmUz2y2FhYTCb\nzQhxGO6ckZGB8+fP44477sCaNWsgdRKMoaFB0Omcr6XcUwaDHg1Xe6jja/IAAIH3JSPQ0Ml5Syqg\nc2j9iAh5WSyDymvuDm6L+vjKdgDcFrXylW1Rw3Z0eyYxIdrv6a1atQrTp0/HoEGDsHz5cuTm5mL2\n7NnXfXxVVe8ey5WPD9bh9OkAAP4I++5TAEDlhMmwmut69bV6m9UKAPIfgb9/E4BAmFVec1cp74sv\n8JVt8ZXtALgtauUr2+KO7ehJ4Lvs4jYajbBYLPbL5eXlMBgM9ssPP/wwhgwZAp1OhxkzZuDkyZPd\nLqI3yF3cAqEFX8A6dBisMbEeqaM7tNq2c205SIyIiBy5DOiEhATk5uYCAIqKimA0Gu3d23V1dVi6\ndCmam5sBAEeOHMGoUaPcWG57UkUFsGoVQn72HP7tyDJ8IP0rdBVmeZYwlR9/ViinWvEYNBEROXLZ\nxR0fHw+TyYTU1FRIkoSMjAzk5ORAr9cjOTkZM2bMQEpKCgICAnDbbbd12r3d23QnjgNbtmAAgIcc\nrr9y/4N9VsONGjhQ4OJF1xOVEBFR/9KlY9AvvPBCu8tjxoyx/3vx4sVYvHhx71bVRS0J04FLl2Ap\nLcP06cEYOdKG9z8QsA0d5pF6esJdSyUSEZF38/7lJsPDUVUbhBPNetwc1Qrb0MuerqhblC5ungdN\nRESOvD+g4TBJidH7FpJYsKAFUVG26y4jSURE/ZOPBHTbNJ/eZs6cVsyZ0+rpMoiISGV8YrEMZaGM\n3lwKkoiIyJN8IqCrq+WA5nFcIiLyFT4R0HV1ckA7Lt9IRETkzXwioGtr5f/rPT91KhERUa/wkYDm\nHjQREfkWnwhodnETEZGvYUATERGpkE8EdG0tIEkCwcGeroSIiKh3+EhASwgJATQ+sTVEREQ+EtB1\ndRK7t4mIyKf4REDX1jKgiYjIt3h9QAsB1NUBej0DmoiIfIfXB3R9PWCzSRg40NOVEBER9R6vD+ia\nGvn/7OImIiJf4vUB3TbNJwOaiIh8h9cHNPegiYjIF3UpoLOyspCSkoLU1FQcO3bM6X02b96MhQsX\n9mpxXdEW0H3+0kRERG7jMqAPHz6M0tJSZGdnIzMzE5mZmR3uU1xcjCNHjrilQFeUgA4J4R40ERH5\nDpcBnZeXh6SkJABAbGwsampqUF9f3+4+mzZtwvPPP++eCl1gFzcREfkinas7WCwWmEwm++WwsDCY\nzWaEhIQAAHJycjBlyhQMHz68Sy8YGhoEnU7bw3I7UgI6KmoADIZee1qPMRh8Z1Frbov6+Mp2ANwW\ntfKVbVHDdrgM6GsJ0banWl1djZycHLz33nsoKyvr0uOrqhq7+5KdqqnRX62rEWaztVefu68ZDHqY\nzXWeLqNXcFvUx1e2A+C2qJWvbIs7tqMnge+yi9toNMJisdgvl5eXw3B1VzU/Px+VlZVYsGABVqxY\ngaKiImRlZXW7iBuh7EHzNCsiIvIlLgM6ISEBubm5AICioiIYjUZ79/bs2bOxf/9+7NmzB1u3boXJ\nZEJ6erp7K74Gj0ETEZEvctnFHR8fD5PJhNTUVEiShIyMDOTk5ECv1yM5ObkvauyUMlEJA5qIiHxJ\nl45Bv/DCC+0ujxkzpsN9IiMjsWPHjt6pqhvaurj7/KWJiIjcxidmEgsOFtD23sBwIiIij/OJgGb3\nNhER+RqfCGiO4CYiIl/j1QEthBLQnq6EiIiod3l1QF++DLS2soubiIh8j1cHdF2dBIABTUREvser\nA7q2lgFNRES+yasDuu7qVKk8Bk1ERL7GqwOae9BEROSrvDqgeQyaiIh8lVcHtM0m/99gYEATEZFv\n6fZ60Gpy332tyMkBJk9u9XQpREREvcqr96ADA4FHHgECAjxdCRERUe/y6oAmIiLyVQxoIiIiFWJA\nExERqRADmoiISIUkIQTPUSIiIlIZ7kETERGpEAOaiIhIhRjQREREKsSAJiIiUiEGNBERkQoxoImI\niFTIqxfLyMrKQkFBASRJQnp6OsaPH+/pkrrl9ddfx9GjR9Ha2opnnnkGn332GYqKijB48GAAwNKl\nS3H33Xd7tsguOHToEH72s59h1KhRAIDRo0fjqaeewtq1a2G1WmEwGPDGG2/A39/fw5W69uGHH2Lf\nvn32y4WFhYiLi0NjYyOCgoIAAC+++CLi4uI8VaJLJ0+exHPPPYcnnngCaWlpuHjxotP3Yt++ffjD\nH/4AjUaDefPm4fHHH/d06e04247169ejtbUVOp0Ob7zxBgwGA0wmE+Lj4+2Pe//996HVaj1YeUfX\nbsu6deucftbV/p4AHbdl1apVqKqqAgBUV1fj9ttvxzPPPIM5c+bYPyehoaF46623PFl2B9d+/44b\nN059nxPhpQ4dOiSWLVsmhBCiuLhYzJs3z8MVdU9eXp546qmnhBBCVFZWipkzZ4oXX3xRfPbZZx6u\nrPvy8/PFypUr2123bt06sX//fiGEEJs3bxa7du3yRGk35NChQ2Ljxo0iLS1N/PDDD54up0saGhpE\nWlqaeOmll8SOHTuEEM7fi4aGBnHvvfeK2tpacfnyZfHggw+KqqoqT5bejrPtWLt2rfjLX/4ihBBi\n586d4rXXXhNCCDFlyhSP1dkVzrbF2Wdd7e+JEM63xdG6detEQUGBOHv2rHjkkUc8UGHXOPv+VePn\nxGu7uPPy8pCU7QqqDwAAFIRJREFUlAQAiI2NRU1NDerr6z1cVddNnjwZv/3tbwEAAwcOxOXLl2G1\nWj1cVe85dOgQ7rnnHgDArFmzkJeX5+GKum/btm147rnnPF1Gt/j7+2P79u0wGo3265y9FwUFBRg3\nbhz0ej0CAwMRHx+Pb775xlNld+BsOzIyMnDfffcBkPfIqqurPVVetzjbFmfU/p4AnW/L6dOnUVdX\n5xU9mc6+f9X4OfHagLZYLAgNDbVfDgsLg9ls9mBF3aPVau1dpnv37sWMGTOg1Wqxc+dOLFq0CM8/\n/zwqKys9XGXXFRcX49lnn8X8+fNx8OBBXL582d6lPWTIEK96bwDg2LFjGDZsGAwGAwDgrbfewoIF\nC7BhwwY0NTV5uLrr0+l0CAwMbHeds/fCYrEgLCzMfh+1fX6cbUdQUBC0Wi2sViv+9Kc/Yc6cOQCA\n5uZmrFmzBqmpqXjvvfc8UW6nnG0LgA6fdbW/J8D1twUA/vjHPyItLc1+2WKxYNWqVUhNTW132EgN\nnH3/qvFz4tXHoB0JL52x9NNPP8XevXvx+9//HoWFhRg8eDDGjh2L3/3ud9i6dSs2bNjg6RJdGjFi\nBFasWIH7778fZ8+exaJFi9r1Bnjje7N371488sgjAIBFixbh1ltvRXR0NDIyMrBr1y4sXbrUwxX2\nzPXeC295j6xWK9auXYs777wTU6dOBQCsXbsWDz30ECRJQlpaGiZNmoRx48Z5uNLO/cu//EuHz/rE\niRPb3cdb3hNA/pF09OhRbNy4EQAwePBg/OxnP8NDDz2Euro6PP7447jzzjtd9iL0Ncfv33vvvdd+\nvVo+J167B200GmGxWOyXy8vL7Xs73uKLL77A22+/je3bt0Ov12Pq1KkYO3YsACAxMREnT570cIVd\nEx4ejgceeACSJCE6Oho33XQTampq7HuaZWVlqvtgunLo0CH7F2ZycjKio6MBeNf7oggKCurwXjj7\n/HjDe7R+/XrcfPPNWLFihf26+fPnIzg4GEFBQbjzzju94v1x9ln31vcEAI4cOdKuazskJASPPfYY\n/Pz8EBYWhri4OJw+fdqDFXZ07fevGj8nXhvQCQkJyM3NBQAUFRXBaDQiJCTEw1V1XV1dHV5//XW8\n88479pGcK1euxNmzZwHIAaGMila7ffv24d133wUAmM1mVFRU4NFHH7W/P5988gmmT5/uyRK7pays\nDMHBwfD394cQAk888QRqa2sBeNf7orjrrrs6vBcTJkzAd999h9raWjQ0NOCbb77BpEmTPFxp5/bt\n2wc/Pz+sWrXKft3p06exZs0aCCHQ2tqKb775xiveH2efdW98TxTfffcdxowZY7+cn5+PV199FQDQ\n2NiIEydOICYmxlPldeDs+1eNnxOv7eKOj4+HyWRCamoqJElCRkaGp0vqlv3796OqqgqrV6+2X/fo\no49i9erVGDBgAIKCgux/4GqXmJiIF154AX/729/Q0tKCjRs3YuzYsXjxxReRnZ2NiIgIPPzww54u\ns8vMZrP9uJMkSZg3bx6eeOIJDBgwAOHh4Vi5cqWHK7y+wsJCvPbaazh//jx0Oh1yc3Px5ptvYt26\nde3eCz8/P6xZswZLly6FJElYvnw59Hq9p8u3c7YdFRUVCAgIwMKFCwHIg0M3btyIoUOHYu7cudBo\nNEhMTFTdICVn25KWltbhsx4YGKjq9wRwvi1btmyB2Wy29zIBwKRJk/DRRx8hJSUFVqsVy5YtQ3h4\nuAcrb8/Z9++mTZvw0ksvqepzwuUmiYiIVMhru7iJiIh8GQOaiIhIhRjQREREKsSAJiIiUiEGNBER\nkQoxoImIiFSIAU1ERKRCXQrokydPIikpCTt37uxw21dffYW5c+ciJSUF27Zt6/UCiYiI+iOXAd3Y\n2IiXX37ZPjH9tV555RVs2bIFu3fvxsGDB1FcXNzrRRIREfU3LgO6s/U/z549i0GDBmHYsGHQaDSY\nOXOmV677S0REpDYuA7qz9T8d5ywG1Ll+KRERkTfq80FinPqbiIjItRtazeratTK7su6vJEkwm+tu\n5GXJBYNBzzbuA2xn92Mbux/buG8YDN1fBeuG9qAjIyNRX1+Pc+fOobW1FZ9//jkSEhJu5CmJiIgI\nXdiDdrb+Z2JiIiIjI5GcnIyNGzdizZo1AIAHHnhAVYtyExEReSuPrAfN7hT3YpdV32A7ux/b2P3Y\nxn2jz7u4iYiIyD0Y0ERERCrEgCYiIlIhBjQREZEKMaCJiIhUiAFNRESkQgxoIiIiFWJAExERqRAD\nmoiISIUY0ERERCrEgCYiIlIhBjQREZEKMaCJiIhUiAFNRESkQgxoIiIiFWJAExERqRADmoiISIUY\n0ERERCrEgCYiIlIhXVfulJWVhYKCAkiShPT0dIwfP95+265du7Bv3z5oNBrExcXhF7/4hduKJSIi\n6i9c7kEfPnwYpaWlyM7ORmZmJjIzM+231dfX491338WuXbuwe/dulJSU4B//+IdbCyYiIuoPXAZ0\nXl4ekpKSAACxsbGoqalBfX09AMDPzw9+fn5obGxEa2srLl++jEGDBrm3YiIion7AZRe3xWKByWSy\nXw4LC4PZbEZISAgCAgKwfPlyJCUlISAgAA8++CBiYmJcvqjBoL+xqskltnHfYDu7H9vY/djG6tSl\nY9COhBD2f9fX1+Odd97Bxx9/jJCQECxevBgnTpzAmDFjOn0Os7mu+5VSlxkMerZxH2A7ux/b2P3Y\nxn2jJz+CXHZxG41GWCwW++Xy8nIYDAYAQElJCaKiohAWFgZ/f39MmjQJhYWF3S6CiIiI2nMZ0AkJ\nCcjNzQUAFBUVwWg0IiQkBAAwfPhwlJSUoKmpCQBQWFiIESNGuK9aIiKifsJlF3d8fDxMJhNSU1Mh\nSRIyMjKQk5MDvV6P5ORkLF26FIsWLYJWq8XEiRMxadKkvqibiIjIp0nC8aByH+HxDvfiMaW+wXZ2\nP7ax+7GN+4ZbjkETERFR32NAExERqRADmoiISIUY0ERERCrEgCYiIlIhBjQREZEKMaCJiIhUiAFN\nRESkQgxoIiIiFWJAExERqRADmoiISIUY0ERERCrEgCYiIlIhBjQREZEKMaCJiIhUiAFNRESkQgxo\nIiIiFWJAExERqZCuK3fKyspCQUEBJElCeno6xo8fb7/t4sWL+PnPf46Wlhbcdttt+PWvf+22YomI\niPoLl3vQhw8fRmlpKbKzs5GZmYnMzMx2t2/atAlPPvkk9u7dC61WiwsXLritWCIiov7CZUDn5eUh\nKSkJABAbG4uamhrU19cDAGw2G44ePYrExEQAQEZGBiIiItxYLhERUf/gsovbYrHAZDLZL4eFhcFs\nNiMkJASVlZUIDg7Gq6++iqKiIkyaNAlr1qxx+aIGg/7GqiaX2MZ9g+3sfmxj92Mbq1OXjkE7EkK0\n+3dZWRkWLVqE4cOHY9myZThw4ADuvvvuTp/DbK7rdqHUdQaDnm3cB9jO7sc2dj+2cd/oyY8gl13c\nRqMRFovFfrm8vBwGgwEAEBoaioiICERHR0Or1WLq1Kk4depUt4sgIiKi9lwGdEJCAnJzcwEARUVF\nMBqNCAkJAQDodDpERUXhzJkz9ttjYmLcVy0REVE/4bKLOz4+HiaTCampqZAkCRkZGcjJyYFer0dy\ncjLS09Oxbt06CCEwevRo+4AxIiIi6jlJOB5U7iM83uFePKbUN9jO7sc2dj+2cd9wyzFoIiIi6nsM\naCIiIhViQBMREakQA5qIiEiFGNBEREQqxIAmIiJSIQY0ERGRCjGgiYiIVIgBTUREpEIMaCIiIhVi\nQBMREakQA5qIiEiFGNBEREQqxIAmIiJSIQY0ERGRCjGgiYiIVIgBTUREpEIMaCIiIhXqUkBnZWUh\nJSUFqampOHbsmNP7bN68GQsXLuzV4oiIiPorlwF9+PBhlJaWIjs7G5mZmcjMzOxwn+LiYhw5csQt\nBRIREfVHLgM6Ly8PSUlJAIDY2FjU1NSgvr6+3X02bdqE559/3j0VEhER9UM6V3ewWCwwmUz2y2Fh\nYTCbzQgJCQEA5OTkYMqUKRg+fHiXX9Rg0PegVOoOtnHfYDu7H9vY/djG6uQyoK8lhLD/u7q6Gjk5\nOXjvvfdQVlbW5ecwm+u6+7LUDQaDnm3cB9jO7sc2dj+2cd/oyY8gl13cRqMRFovFfrm8vBwGgwEA\nkJ+fj8rKSixYsAArVqxAUVERsrKyul0EERERtecyoBMSEpCbmwsAKCoqgtFotHdvz549G/v378ee\nPXuwdetWmEwmpKenu7diIiKifsBlF3d8fDxMJhNSU1MhSRIyMjKQk5MDvV6P5OTkvqiRiIio35GE\n40HlPsLjHe7FY0p9g+3sfmxj92Mb9w23HIMmIiKivseAJiIiUiEGNBERkQoxoImIiFSIAU1ERKRC\nDGgiIiIVYkATERGpEAOaiIhIhRjQREREKsSAJiIiUiEGNBERkQoxoImIiFSIAU1ERKRCDGgiIiIV\nYkATERGpEAOaiIhIhRjQREREKsSAJiIiUiFdV+6UlZWFgoICSJKE9PR0jB8/3n5bfn4+fvOb30Cj\n0SAmJgaZmZnQaJj7REREN8Jlkh4+fBilpaXIzs5GZmYmMjMz292+YcMGvPXWW/jggw/Q0NCAL774\nwm3FEhER9RcuAzovLw9JSUkAgNjYWNTU1KC+vt5+e05ODoYOHQoACAsLQ1VVlZtKJSIi6j9cBrTF\nYkFoaKj9clhYGMxms/1ySEgIAKC8vBwHDx7EzJkz3VAmERFR/9KlY9COhBAdrquoqMCzzz6LjIyM\ndmF+PQaDvrsvS93ENu4bbGf3Yxu7H9tYnVwGtNFohMVisV8uLy+HwWCwX66vr8fTTz+N1atXY9q0\naV16UbO5rgelUlcZDHq2cR9gO7sf29j92MZ9oyc/glx2cSckJCA3NxcAUFRUBKPRaO/WBoBNmzZh\n8eLFmDFjRrdfnIiIiJxzuQcdHx8Pk8mE1NRUSJKEjIwM5OTkQK/XY9q0afjoo49QWlqKvXv3AgB+\n+tOfIiUlxe2FExER+TJJODuo7GbsTnEvdln1Dbaz+7GN3Y9t3Dfc0sVNREREfY8BTUREpEIMaCIi\nIhViQBMREakQA5qIiEiFGNBEREQqxIAmIiJSIQY0ERGRCjGgiYiIVIgBTUREpEIMaCIiIhViQBMR\nEakQA5qIiEiFGNBEREQqxIAmIiJSIQY0ERGRCjGgiYiIVIgBTUREpEJdCuisrCykpKQgNTUVx44d\na3fbV199hblz5yIlJQXbtm1zS5FERET9jcuAPnz4MEpLS5GdnY3MzExkZma2u/2VV17Bli1bsHv3\nbhw8eBDFxcVuK5aIiKi/cBnQeXl5SEpKAgDExsaipqYG9fX1AICzZ89i0KBBGDZsGDQaDWbOnIm8\nvDz3VkxERNQPuAxoi8WC0NBQ++WwsDCYzWYAgNlsRlhYmNPbiIiIqOd03X2AEOKGX9Rg0N/wc1Dn\n2MZ9g+3sfmxj92Mbq5PLPWij0QiLxWK/XF5eDoPB4PS2srIyGI1GN5RJRETUv7gM6ISEBOTm5gIA\nioqKYDQaERISAgCIjIxEfX09zp07h9bWVnz++edISEhwb8VERET9gCS60Gf95ptv4uuvv4YkScjI\nyMDx48eh1+uRnJyMI0eO4M033wQA3HvvvVi6dKnbiyYiIvJ1XQpoIiIi6lucSYyIiEiFGNBEREQq\n5NaA5hSh7tdZG+fn52PevHlITU3F+vXrYbPZPFSld+usjRWbN2/GwoUL+7gy39FZG1+8eBHz58/H\n3LlzsWHDBg9V6Bs6a+ddu3YhJSUF8+fP7zBjJHXdyZMnkZSUhJ07d3a4rdu5J9zk0KFDYtmyZUII\nIYqLi8W8efPa3X7//feLCxcuCKvVKubPny9OnTrlrlJ8lqs2Tk5OFhcvXhRCCLFy5Upx4MCBPq/R\n27lqYyGEOHXqlEhJSRFpaWl9XZ5PcNXGq1atEp988okQQoiNGzeK8+fP93mNvqCzdq6rqxOzZs0S\nLS0tQgghlixZIr799luP1OnNGhoaRFpamnjppZfEjh07Otze3dxz2x40pwh1v87aGABycnIwdOhQ\nAPIsb1VVVR6p05u5amMA2LRpE55//nlPlOcTOmtjm82Go0ePIjExEQCQkZGBiIgIj9XqzTprZz8/\nP/j5+aGxsRGtra24fPkyBg0a5MlyvZK/vz+2b9/udD6QnuSe2wKaU4S6X2dtDMB+vnp5eTkOHjyI\nmTNn9nmN3s5VG+fk5GDKlCkYPny4J8rzCZ21cWVlJYKDg/Hqq69i/vz52Lx5s6fK9HqdtXNAQACW\nL1+OpKQkzJo1CxMmTEBMTIynSvVaOp0OgYGBTm/rSe712SAxwbO53M5ZG1dUVODZZ59FRkZGuw8n\n9YxjG1dXVyMnJwdLlizxYEW+x7GNhRAoKyvDokWLsHPnThw/fhwHDhzwXHE+xLGd6+vr8c477+Dj\njz/G3/72NxQUFODEiRMerI4ANwY0pwh1v87aGJA/dE8//TRWr16NadOmeaJEr9dZG+fn56OyshIL\nFizAihUrUFRUhKysLE+V6rU6a+PQ0FBEREQgOjoaWq0WU6dOxalTpzxVqlfrrJ1LSkoQFRWFsLAw\n+Pv7Y9KkSSgsLPRUqT6pJ7nntoDmFKHu11kbA/Kx0cWLF2PGjBmeKtHrddbGs2fPxv79+7Fnzx5s\n3boVJpMJ6enpnizXK3XWxjqdDlFRUThz5oz9dna99kxn7Tx8+HCUlJSgqakJAFBYWIgRI0Z4qlSf\n1JPcc+tMYpwi1P2u18bTpk3D5MmTMXHiRPt9f/rTnyIlJcWD1Xqnzv6OFefOncP69euxY8cOD1bq\nvTpr49LSUqxbtw5CCIwePRobN26ERsMpHHqis3b+4IMPkJOTA61Wi4kTJ2Lt2rWeLtfrFBYW4rXX\nXsP58+eh0+kQHh6OxMREREZG9ij3ONUnERGRCvFnKBERkQoxoImIiFSIAU1ERKRCDGgiIiIVYkAT\nERGpEAOaiIhIhRjQREREKsSAJiIiUqH/D/NUfiL+q2dRAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "pKE6dtuK_F7d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing the model on the entire dataset with the best weights"
      ]
    },
    {
      "metadata": {
        "id": "tHSVVCA8p8wq",
        "colab_type": "code",
        "outputId": "e8860821-358c-4c21-d5da-ffc42348bc84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "#Loading Check-Pointed\n",
        "#The checkpoint only includes the model weights. It assumes you know the network structure.\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(5,input_shape=(908,), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# load weights\n",
        "model.load_weights(\"weights.best.hdf5\")\n",
        "\n",
        "#Compile\n",
        "model.compile('RMSprop', 'categorical_crossentropy', metrics=['accuracy'])\n",
        "print(\"Created model and loaded weights from file\")\n",
        "\n",
        "# estimate accuracy on whole dataset using loaded weights\n",
        "scores = model.evaluate(train_features, train_labels, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created model and loaded weights from file\n",
            "acc: 83.81%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dh2siqkT_MO0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing testing data"
      ]
    },
    {
      "metadata": {
        "id": "341iHUOtQTwS",
        "colab_type": "code",
        "outputId": "28130936-1338-407d-fef2-9c9ce3e6a175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#Load testing data\n",
        "testing_data=pd.read_csv('test_features.csv', header=None)\n",
        "print('Test dataset shape is: ' + str(testing_data.shape))\n"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test dataset shape is: (560, 903)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5X58RLkDtDU_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get one hot encoding of columns 1-3 in the testing dataset\n",
        "one_hot = pd.get_dummies(testing_data.iloc[:,:3])\n",
        "\n",
        "# Drop columns 1-3 as it is now encoded\n",
        "testing_data = testing_data.drop(testing_data.iloc[:,:3],axis = 1)\n",
        "\n",
        "# Join the encoded df\n",
        "testing_data = testing_data.join(one_hot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iN69U-Gt_Rph",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ]
    },
    {
      "metadata": {
        "id": "YerEguaCrJGn",
        "colab_type": "code",
        "outputId": "79613b8d-8754-4057-901c-f4f072a7eeae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#After the model has been train we can make predictions\n",
        "\n",
        "y_pred = model.predict(testing_data)\n",
        "y_pred.shape"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(560, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "metadata": {
        "id": "RA0p-g1MsBMR",
        "colab_type": "code",
        "outputId": "12f88e36-c71e-48d5-cc5e-b2c5e2baf50c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.2113693e-04, 9.9962056e-01, 5.8235433e-05],\n",
              "       [2.6405293e-03, 9.9651033e-01, 8.4917585e-04],\n",
              "       [1.3229930e-01, 7.3836726e-01, 1.2933338e-01],\n",
              "       ...,\n",
              "       [2.0889652e-01, 3.5126781e-01, 4.3983561e-01],\n",
              "       [2.0889652e-01, 3.5126781e-01, 4.3983561e-01],\n",
              "       [2.0889652e-01, 3.5126781e-01, 4.3983561e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "metadata": {
        "id": "EWOJOphP_aYx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convert the probabilities to values\n"
      ]
    },
    {
      "metadata": {
        "id": "vGWpNrJry3c2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "answers=[]\n",
        "i=0\n",
        "\n",
        "for row in y_pred:\n",
        "  max_value = max(y_pred[i])\n",
        "  max_index = list(y_pred[i]).index(max_value)\n",
        "  answers.append(max_index)\n",
        "  i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZpQN2lau_hvv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check that we got 560 answers"
      ]
    },
    {
      "metadata": {
        "id": "6rzJ1Nkq1Qy1",
        "colab_type": "code",
        "outputId": "6463917a-35fa-40de-9fb3-37adc1eb632c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(answers)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "560"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "metadata": {
        "id": "y3_CUz3y_wI4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Explore answers\n"
      ]
    },
    {
      "metadata": {
        "id": "Y0LAAu9r1i5h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "answers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TSg6UbBN_yWb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Save to CSV file\n"
      ]
    },
    {
      "metadata": {
        "id": "oTdNgr8U1kgE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('test_predictions.csv', \"w\") as output:\n",
        "    writer = csv.writer(output, lineterminator='\\n')\n",
        "    for val in answers:\n",
        "        writer.writerow([val])    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2-SREZ5nBHsV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check counts"
      ]
    },
    {
      "metadata": {
        "id": "e1kUqZ2J2FZO",
        "colab_type": "code",
        "outputId": "78f46de5-0f84-4811-c581-8c737d37c268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "my_dict = {i:answers.count(i) for i in answers}\n",
        "print (my_dict)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 489, 2: 68, 0: 3}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}